{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda8002-e229-4373-8431-b4458568218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ray import init, rllib, shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c8c-c3e1-4a6e-9f23-d9974348334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack_simulator.agents import DEFENDERS\n",
    "from attack_simulator.alphazero_env import AttackSimulationAlphaZeroEnv\n",
    "from attack_simulator.env import AttackSimulationEnv\n",
    "from attack_simulator.graph import AttackGraph, SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d59b78-7bc4-48c5-9d53-a0ed7a73f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPolicy(rllib.policy.Policy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "        agent_config = dict(\n",
    "            input_dim=observation_space.shape[0],\n",
    "            num_actions=action_space.n,\n",
    "            random_seed=config[\"seed\"],\n",
    "            attack_graph=config[\"env_config\"][\"attack_graph\"],\n",
    "        )\n",
    "        self._agent = DEFENDERS[config[\"agent_type\"]](agent_config)\n",
    "\n",
    "    def compute_actions(self, observations, *args, **kwargs):\n",
    "        # FIXME: use a `numpy` array as a temporary workaround for\n",
    "        #        https://github.com/ray-project/ray/issues/10100\n",
    "        return np.array([self._agent.act(obs) for obs in observations]), [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        return {}\n",
    "\n",
    "    \n",
    "def template_agent(agent_type):\n",
    "    default_config = rllib.agents.trainer.with_common_config(dict(agent_type=agent_type))\n",
    "    return rllib.agents.trainer_template.build_trainer(\n",
    "        name=agent_type,\n",
    "        default_policy=AgentPolicy,\n",
    "        default_config=default_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397d725-6f0e-43c4-9fc7-5f13e41e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'contrib/AlphaZero' does NOT appear to work without its custom dense model\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "\n",
    "rllib.models.ModelCatalog.register_custom_model(\"alpha_zero_dense_model\", DenseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f262a77-9f87-436a-8208-a3f1bad01b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.client import worker\n",
    "\n",
    "worker.INITIAL_TIMEOUT_SEC = worker.MAX_TIMEOUT_SEC = 1\n",
    "\n",
    "\n",
    "def ray_init():\n",
    "    if os.path.isdir(\"/var/run/secrets/kubernetes.io\") or os.path.exists(\n",
    "        os.path.expanduser(\"~/ray_bootstrap_config.yaml\")\n",
    "    ):\n",
    "        # inside k8s pod or a cluster managed by Ray's autoscaler\n",
    "        context = init(address=\"auto\")\n",
    "    else:\n",
    "        ray_client_server = \"host.docker.internal\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "        try:\n",
    "            context = init(address=f\"ray://{ray_client_server}:10001\")\n",
    "        except ConnectionError:\n",
    "            # clean up after failed connection attempt\n",
    "            shutdown()\n",
    "            # listen on all interfaces inside a container for port-forwarding to work\n",
    "            dashboard_host = \"0.0.0.0\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "            context = init(num_cpus=4, dashboard_host=dashboard_host)\n",
    "    print(\"\\x1b[33;1m\", context, \"\\x1b[m\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8a4c-6aa2-4e41-b86c-e622f9b2c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.utils.trainable import TrainableUtil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "rename = dict(\n",
    "    agent_type=\"Agent\",\n",
    "    graph_size=\"Graph size\",\n",
    "    episode_length=\"Episode lengths\",\n",
    "    episode_reward=\"Returns\",\n",
    ")\n",
    "\n",
    "agent_types = [\"contrib/AlphaZero\", \"R2D2\", \"rule-based\", \"random\"]\n",
    "graphs = [AttackGraph(dict(graph_size=size)) for size in SIZES]\n",
    "seeds = [0, 1, 2, 3, 6, 7, 11, 28, 42, 1337]\n",
    "iterations = 10\n",
    "rollouts = 10\n",
    "\n",
    "train_config = dict(\n",
    "    num_workers=4,\n",
    "    rollout_fragment_length=32,\n",
    "    train_batch_size=640,\n",
    "    buffer_size=512,\n",
    "    batch_mode=\"complete_episodes\",\n",
    ")\n",
    "eval_config = dict(\n",
    "    evaluation_interval=1,\n",
    "    evaluation_num_workers=1,\n",
    "    evaluation_config=dict(explore=False, replay_sequence_length=-1),\n",
    "    evaluation_num_episodes=rollouts,\n",
    ")\n",
    "\n",
    "\n",
    "def generate(savename):\n",
    "    ray_init()\n",
    "\n",
    "    frames = []\n",
    "    for graph in tqdm(graphs, \"graphs\"):\n",
    "        for agent_type in tqdm(agent_types, f\"└── {graph.graph_size}\"):\n",
    "            agent_name = agent_type.split(\"/\")[-1]\n",
    "            for seed in tqdm(seeds, f\"\\u00a0\\u2001\\u2001\\u2001└── {agent_name}@{graph.graph_size}\"):\n",
    "                config = dict(\n",
    "                    framework=\"torch\",\n",
    "                    env=AttackSimulationEnv,\n",
    "                    env_config=dict(attack_graph=graph),\n",
    "                    seed=seed,\n",
    "                    log_level=\"ERROR\",\n",
    "                )\n",
    "                if agent_type in DEFENDERS:\n",
    "                    config.update(eval_config)\n",
    "                    with keep_ipython_sane():\n",
    "                        agent = template_agent(agent_type)(config=config)\n",
    "                else:\n",
    "                    if agent_type == \"contrib/AlphaZero\":\n",
    "                        config[\"env_config\"].update(env_class=AttackSimAlphaZeroEnv)\n",
    "                        config.update(\n",
    "                            env=AlphaZeroWrapper,\n",
    "                            model=dict(custom_model=\"alpha_zero_dense_model\"),\n",
    "                        )\n",
    "                    if agent_type == \"R2D2\":\n",
    "                        config.update(model=dict(use_lstm=True))\n",
    "\n",
    "                    name = f\"{agent_name}_{graph.graph_size}_{seed}\"\n",
    "                    if not os.path.exists(name):\n",
    "                        config.update(train_config)\n",
    "                        with keep_ipython_sane():\n",
    "                            agent = rllib.agents.registry.get_trainer_class(agent_type)(\n",
    "                                config=config\n",
    "                            )\n",
    "                        for _ in tqdm(\n",
    "                            range(iterations),\n",
    "                            f\"\\u00a0\\u2001\\u2001\\u2001\\u2001\\u2001\\u2001└── {name}\",\n",
    "                        ):\n",
    "                            agent.train()\n",
    "                            # TODO: break based on results?\n",
    "                            # results = agent.train()\n",
    "                        agent.save(name)\n",
    "                        del agent\n",
    "\n",
    "                    config.update(eval_config, num_workers=0)\n",
    "                    with keep_ipython_sane():\n",
    "                        agent = rllib.agents.registry.get_trainer_class(agent_type)(config=config)\n",
    "                        checkpoint_path = TrainableUtil.get_checkpoints_paths(name).chkpt_path[0]\n",
    "                        agent.restore(checkpoint_path)\n",
    "\n",
    "                stats = agent.evaluate()[\"evaluation\"][\"hist_stats\"]\n",
    "                frame = pd.DataFrame(\n",
    "                    dict(agent_type=agent_type, graph_size=graph.num_attacks, **stats)\n",
    "                )\n",
    "                frames.append(frame)\n",
    "    shutdown()\n",
    "    results_df = pd.concat(frames, ignore_index=True).rename(columns=rename)\n",
    "    results_df.to_csv(savename)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a1282-533b-4d8a-bb9e-f6c67464b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = \"returns-agent-eval.csv\"\n",
    "\n",
    "df = generate(savename) if not os.path.exists(savename) else pd.read_csv(savename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ae506-5fc6-4fcf-b1fe-91a2df9f12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984bbe-82c6-4dca-8436-6d78cf97d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", rc={\"figure.figsize\": (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5424a47-3486-472b-a604-779d30ca7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Returns\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"lower left\")\n",
    "g.set_title(\"Returns vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2d787-49db-4d90-a8ae-a69d97c085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Episode lengths\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Episode lengths vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6d9d1-2b14-4e48-a4a9-85785a112121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 32)\n",
    "df.groupby(\"Agent\").describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
