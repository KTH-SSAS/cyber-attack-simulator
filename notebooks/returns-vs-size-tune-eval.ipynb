{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fda8002-e229-4373-8431-b4458568218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from ray import init, rllib, tune, shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b148c8c-c3e1-4a6e-9f23-d9974348334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack_simulator.agents import DEFENDERS\n",
    "from attack_simulator.env import AttackSimulationEnv\n",
    "from attack_simulator.graph import AttackGraph, SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d59b78-7bc4-48c5-9d53-a0ed7a73f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class BooleanVectorPreprocessor(rllib.models.preprocessors.Preprocessor):\n",
    "    def _init_shape(self, observation_space, options=None):\n",
    "        return (len(observation_space.spaces),)\n",
    "\n",
    "    def transform(self, observation):\n",
    "        return np.array(observation)\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        space = gym.spaces.Box(0, 1, self.shape, dtype='int8')\n",
    "        space.original_space = self._obs_space\n",
    "        return space\n",
    "\n",
    "rllib.models.ModelCatalog.register_custom_preprocessor('boolean_vector', BooleanVectorPreprocessor)\n",
    "\n",
    "\n",
    "class AgentPolicy(rllib.policy.Policy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "        agent_config = dict(\n",
    "            input_dim=observation_space.shape[0], # same as len(observation_space.original_space.spaces)\n",
    "            num_actions=action_space.n,\n",
    "            random_seed=config['seed'],\n",
    "            attack_graph=config['env_config']['attack_graph'],\n",
    "        )\n",
    "        self._agent = DEFENDERS[config['agent_type']](agent_config)\n",
    "\n",
    "    def compute_actions(self, observations, *args, **kwargs):\n",
    "        return [self._agent.act(obs) for obs in observations], [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def instantiate_agent(agent_type, config):\n",
    "    default_config = rllib.agents.trainer.with_common_config(\n",
    "        dict(config, agent_type=agent_type, env_class=config['env'])  # , model=dict(custom_preprocessor='boolean_vector'))\n",
    "    )\n",
    "    return rllib.agents.trainer_template.build_trainer(\n",
    "        name=agent_type,\n",
    "        default_policy=AgentPolicy,\n",
    "        default_config=default_config,\n",
    "    )(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c41ecb4-e26b-46ad-a2d6-bd09cf3b7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class AlphaZeroWrapper(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        self.env = config['env_class'](config)\n",
    "        self.action_space = self.env.action_space\n",
    "        assert isinstance(self.action_space, gym.spaces.Discrete), 'AlphaZero requires a Discrete action space'\n",
    "        shape = (self.action_space.n,)\n",
    "        self.observation_space = gym.spaces.Dict(dict(obs=self.env.observation_space, action_mask=gym.spaces.Box(0, 1, shape)))\n",
    "        self.reward = 0\n",
    "        self.mask = np.full(shape, 1, dtype='int8')\n",
    "        \n",
    "    def reset(self):\n",
    "        self.reward = 0\n",
    "        observation = self.env.reset()\n",
    "        return dict(obs=observation, action_mask=self.mask)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.reward += reward\n",
    "        reward = self.reward if done else 0\n",
    "        return dict(obs=observation, action_mask=self.mask), reward, done, info\n",
    "\n",
    "    def set_state(self, state):\n",
    "        env, self.reward = state\n",
    "        self.env = deepcopy(env)\n",
    "        return dict(obs=self.env.observation, action_mask=self.mask)\n",
    "\n",
    "    def get_state(self):\n",
    "        return deepcopy(self.env), self.reward\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode=None):\n",
    "        self.env.render(mode)\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c397d725-6f0e-43c4-9fc7-5f13e41e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'contrib/AlphaZero' does NOT appear to work without its custom dense model\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "rllib.models.ModelCatalog.register_custom_model('alpha_zero_dense_model', DenseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7211eb85-4419-46a1-9848-876e074e8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class RolloutAggregator:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._kwargs = kwargs\n",
    "        self._episodes = []\n",
    "\n",
    "    def begin_rollout(self):\n",
    "        self._rewards = []\n",
    "\n",
    "    def append_step(self, obs, action, next_obs, reward, done, info):\n",
    "        self._rewards.append(reward)\n",
    "\n",
    "    def end_rollout(self):\n",
    "        self._episodes.append(\n",
    "            dict(self._kwargs, episode_length=len(self._rewards), episode_reward=sum(self._rewards))\n",
    "        )\n",
    "    \n",
    "    def to_df(self):\n",
    "        return pd.DataFrame(self._episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f262a77-9f87-436a-8208-a3f1bad01b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:33:24,077\tINFO worker.py:825 -- Connecting to existing Ray cluster at address: 10.28.0.130:6379\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('/var/run/secrets/kubernetes.io'):  # inside k8s pod\n",
    "    init(address='auto')\n",
    "else:\n",
    "    # listen on all interfaces inside a container for port-forwarding to work\n",
    "    dashboard_host = '0.0.0.0' if os.path.exists('/.dockerenv') else '127.0.0.1'\n",
    "    init(num_cpus=4, dashboard_host=dashboard_host)\n",
    "\n",
    "# ALTERNATIVE: use the \"Ray client\" to connect to a remote cluster\n",
    "# Unfortunately, JupyterNotebookReporter displays an object reference\n",
    "# <IPython.core.display.HTML object> instead of content...\n",
    "# --- --- ---\n",
    "#\n",
    "# from ray.util.client import worker\n",
    "#\n",
    "# worker.INITIAL_TIMEOUT_SEC = worker.MAX_TIMEOUT_SEC = 1\n",
    "#\n",
    "# ray_client_server = 'host.docker.internal' if os.path.exists(\"/.dockerenv\") else '127.0.0.1'\n",
    "# try:\n",
    "#     init(address=f'ray://{ray_client_server}:10001')\n",
    "# except Connection Error:\n",
    "#     pass  # TODO: try something else..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07fb8a4c-6aa2-4e41-b86c-e622f9b2c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.rollout import rollout\n",
    "from tqdm import tqdm\n",
    "\n",
    "agent_types = ['R2D2', 'contrib/AlphaZero', 'rule-based', 'random']\n",
    "seeds = [0, 1, 2, 3, 6, 7, 11, 28, 42, 1337]\n",
    "training_iterations = 10\n",
    "rollouts = 10\n",
    "\n",
    "def generate(savename):\n",
    "    frames = []\n",
    "\n",
    "    for graph_size in SIZES:\n",
    "        graph = AttackGraph(dict(graph_size=graph_size))\n",
    "\n",
    "        for seed in seeds:\n",
    "            for agent_type in agent_types:\n",
    "                config = dict(\n",
    "                    framework='torch',\n",
    "                    model=dict(use_lstm=True),\n",
    "                    env=AttackSimulationEnv,\n",
    "                    env_config=dict(attack_graph=graph),\n",
    "                    seed=seed,\n",
    "                    create_env_on_driver=True,  # apparently, assumed by `rollout`\n",
    "                    num_workers=0,\n",
    "                    batch_mode='complete_episodes',\n",
    "                    log_level='DEBUG',\n",
    "                )\n",
    "                if agent_type in DEFENDERS:\n",
    "                    agent = instantiate_agent(agent_type, config)\n",
    "                else:\n",
    "                    if agent_type == 'contrib/AlphaZero':\n",
    "                        config['env_config'].update(env_class=config['env'])\n",
    "                        config.update(env=AlphaZeroWrapper, model=dict(custom_model='alpha_zero_dense_model'))\n",
    "                    agent_class = rllib.agents.registry.get_trainer_class(agent_type)\n",
    "                    print(agent_class, config)\n",
    "                    agent = agent_class(config=config)\n",
    "                    name = f'{agent_type.split(\"/\")[-1]}_{graph_size}_{seed}'\n",
    "                    if os.path.exists(name):\n",
    "                        checkpoint_path = tune.utils.trainable.TrainableUtil.get_checkpoints_paths(name).chkpt_path[0]\n",
    "                        agent.restore(checkpoint_path)\n",
    "                    else:\n",
    "                        pbar = tqdm(range(training_iterations), name)\n",
    "                        for _ in pbar:\n",
    "                            results = agent.train()\n",
    "                            # TODO: break based on results?\n",
    "                        agent.save(name)\n",
    "\n",
    "                aggregator = RolloutAggregator(agent_type=agent_type, graph_size=graph.num_attacks)\n",
    "                rollout(agent, 'AttackSimulator', num_steps=0, num_episodes=rollouts, saver=aggregator)\n",
    "                frames.append(aggregator.to_df())\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True).rename(columns=dict(agent_type='Agent', graph_size='Graph size', episode_length='Episode lengths', episode_reward='Returns'))\n",
    "    df.to_csv(savename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a1282-533b-4d8a-bb9e-f6c67464b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:33:24,360\tDEBUG rollout_worker.py:1357 -- Creating policy for default_policy\n",
      "2021-09-15 11:33:24,363\tDEBUG catalog.py:707 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f5c5446fa30>: Box(0, 1, (9,), int8) -> (9,)\n",
      "2021-09-15 11:33:24,409\tINFO torch_policy.py:145 -- TorchPolicy (worker=local) running on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttackGraph(en2720.yaml[tiny], 2 services, 7 attack steps)\n",
      "<class 'ray.rllib.agents.trainer_template.R2D2'> {'framework': 'torch', 'model': {'use_lstm': True}, 'env': <class 'attack_simulator.env.AttackSimulationEnv'>, 'env_config': {'attack_graph': <attack_simulator.graph.AttackGraph object at 0x7f5c544d7850>}, 'seed': 0, 'create_env_on_driver': True, 'num_workers': 0, 'batch_mode': 'complete_episodes', 'log_level': 'DEBUG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:33:24,588\tINFO rollout_worker.py:1379 -- Built policy map: {}\n",
      "2021-09-15 11:33:24,591\tINFO rollout_worker.py:1380 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f5c5446fa30>}\n",
      "2021-09-15 11:33:24,592\tINFO rollout_worker.py:611 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f5c5434f550>}\n",
      "2021-09-15 11:33:24,594\tDEBUG rollout_worker.py:716 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f5c5434c7f0> (<AttackSimulationEnv instance>), policies {}\n",
      "2021-09-15 11:33:24,595\tINFO replay_buffer.py:335 -- Since replay_sequence_length=20 and replay_batch_size=1280, we will replay 64 sequences at a time.\n",
      "2021-09-15 11:33:24,597\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "R2D2_tiny_0:   0% 0/10 [00:00<?, ?it/s]2021-09-15 11:33:24,603\tINFO rollout_worker.py:741 -- Generating sample batch of size 4\n",
      "2021-09-15 11:33:24,605\tDEBUG sampler.py:538 -- No episode horizon specified, assuming inf.\n",
      "2021-09-15 11:33:24,609\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}\n",
      "2021-09-15 11:33:24,611\tINFO sampler.py:594 -- Info return from env: {0: {'agent0': None}}\n",
      "2021-09-15 11:33:24,613\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)\n",
      "2021-09-15 11:33:24,614\tINFO sampler.py:827 -- Filtered obs: np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)\n",
      "2021-09-15 11:33:24,616\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': None,\n",
      "                                  'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222),\n",
      "                                  'prev_action': None,\n",
      "                                  'prev_reward': None,\n",
      "                                  'rnn_state': None},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:33:24,630\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                      [ np.ndarray((1, 256), dtype=float32, min=-0.131, max=0.123, mean=-0.003),\n",
      "                        np.ndarray((1, 256), dtype=float32, min=-0.264, max=0.236, mean=-0.006)],\n",
      "                      { 'action_dist_inputs': np.ndarray((1, 3), dtype=float32, min=-0.039, max=-0.013, mean=-0.027),\n",
      "                        'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "                        'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "                        'q_values': np.ndarray((1, 3), dtype=float32, min=-0.039, max=-0.013, mean=-0.027)})}\n",
      "\n",
      "2021-09-15 11:33:24,958\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((62,), dtype=int64, min=0.0, max=2.0, mean=1.113),\n",
      "              'agent_index': np.ndarray((62,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((62,), dtype=float32, min=0.0, max=1.0, mean=0.016),\n",
      "              'eps_id': np.ndarray((62,), dtype=int64, min=1627694678.0, max=1627694678.0, mean=1627694678.0),\n",
      "              'infos': np.ndarray((62,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'obs': np.ndarray((62, 9), dtype=int64, min=0.0, max=1.0, mean=0.224),\n",
      "              'rewards': np.ndarray((62,), dtype=float32, min=0.0, max=2.0, mean=0.032),\n",
      "              'state_in_0': np.ndarray((4, 256), dtype=float32, min=-0.265, max=0.253, mean=-0.005),\n",
      "              'state_in_1': np.ndarray((4, 256), dtype=float32, min=-0.538, max=0.572, mean=-0.012),\n",
      "              'state_out_0': np.ndarray((62, 256), dtype=float32, min=-0.265, max=0.254, mean=-0.006),\n",
      "              'state_out_1': np.ndarray((62, 256), dtype=float32, min=-0.538, max=0.572, mean=-0.016),\n",
      "              'unroll_id': np.ndarray((62,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'weights': np.ndarray((62,), dtype=float32, min=1.0, max=1.0, mean=1.0)}}\n",
      "\n",
      "2021-09-15 11:33:24,962\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((62,), dtype=int64, min=0.0, max=2.0, mean=1.113),\n",
      "  'agent_index': np.ndarray((62,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((62,), dtype=float32, min=0.0, max=1.0, mean=0.016),\n",
      "  'eps_id': np.ndarray((62,), dtype=int64, min=1627694678.0, max=1627694678.0, mean=1627694678.0),\n",
      "  'infos': np.ndarray((62,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'obs': np.ndarray((62, 9), dtype=int64, min=0.0, max=1.0, mean=0.224),\n",
      "  'rewards': np.ndarray((62,), dtype=float32, min=0.0, max=2.0, mean=0.032),\n",
      "  'seq_lens': np.ndarray((4,), dtype=int32, min=2.0, max=20.0, mean=15.5),\n",
      "  'state_in_0': np.ndarray((4, 256), dtype=float32, min=-0.265, max=0.253, mean=-0.005),\n",
      "  'state_in_1': np.ndarray((4, 256), dtype=float32, min=-0.538, max=0.572, mean=-0.012),\n",
      "  'state_out_0': np.ndarray((62, 256), dtype=float32, min=-0.265, max=0.254, mean=-0.006),\n",
      "  'state_out_1': np.ndarray((62, 256), dtype=float32, min=-0.538, max=0.572, mean=-0.016),\n",
      "  'unroll_id': np.ndarray((62,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'weights': np.ndarray((62,), dtype=float32, min=1.0, max=1.0, mean=1.0)}\n",
      "\n",
      "2021-09-15 11:33:25,004\tINFO replay_buffer.py:46 -- Estimated max memory usage for replay buffer is 0.02266 GB (5000.0 batches of size 20, 4532 bytes each), available system memory is 33.678143488 GB\n",
      "2021-09-15 11:33:28,120\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  10% 1/10 [00:03<00:34,  3.80s/it]2021-09-15 11:33:28,498\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:28,780\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:29,015\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:29,239\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:29,508\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:29,797\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:30,149\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:30,395\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:30,649\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:30,878\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:31,128\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:31,376\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:31,606\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:31,845\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:32,136\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:32,367\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:32,634\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:32,853\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:33,120\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:33,384\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:33,617\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:33,866\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:34,271\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:34,514\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:34,743\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:34,994\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:35,215\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:35,501\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:35,749\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:36,013\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:36,232\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:36,479\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:36,721\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:36,958\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:37,193\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:37,437\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:37,705\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:37,952\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:38,196\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:38,427\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:38,714\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:38,988\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:39,264\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:39,538\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:39,796\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:40,039\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:40,287\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:40,504\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:40,718\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:41,010\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:41,280\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:41,514\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:41,771\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:41,997\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:42,236\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:42,473\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:42,702\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:42,943\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:43,172\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:43,444\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:43,745\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:44,079\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:44,328\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:44,583\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:44,875\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:45,124\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:45,354\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:45,589\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:45,832\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:46,081\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:46,301\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:46,547\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:46,827\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  20% 2/10 [00:22<01:40, 12.55s/it]2021-09-15 11:33:47,121\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:47,461\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:47,746\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:47,999\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:48,238\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:48,490\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:48,733\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:49,050\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:49,307\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:49,569\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:49,827\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:50,111\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:50,388\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:50,630\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:50,875\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:51,124\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:51,398\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:51,638\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:51,886\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:52,127\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:52,365\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:52,658\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:52,922\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:53,158\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:53,465\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:53,733\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:54,026\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:54,253\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:54,577\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:54,837\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:55,068\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:55,320\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:55,561\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:55,873\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:56,126\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:56,354\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:56,586\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:56,830\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:57,174\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:57,409\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:57,668\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:57,941\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:58,176\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:58,469\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:58,745\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:58,971\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:59,325\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:59,609\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:33:59,854\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:00,129\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:00,420\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:00,674\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:00,895\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:01,155\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:01,416\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:01,739\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:01,958\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:02,309\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:02,532\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:02,781\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:03,025\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:03,260\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:03,513\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:03,743\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:03,973\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:04,223\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  30% 3/10 [00:39<01:43, 14.75s/it]2021-09-15 11:34:04,474\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:04,765\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:05,008\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:05,251\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:05,479\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:05,711\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:05,937\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:06,201\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:06,476\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:06,721\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:06,958\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:07,207\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:07,495\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:07,762\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:08,033\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:08,292\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:08,530\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:08,761\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:08,997\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:09,238\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:09,509\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:09,747\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:09,990\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:10,263\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:10,505\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:10,736\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:10,954\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:11,220\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:11,477\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:11,807\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:12,033\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:12,262\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:12,505\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:12,762\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:13,002\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:13,249\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:13,498\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:13,800\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:14,037\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:14,300\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:14,603\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:14,835\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:15,161\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:15,400\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:15,660\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:15,922\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:16,254\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:16,499\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:16,718\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:16,937\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:17,159\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:17,389\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:17,647\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:17,918\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:18,243\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:18,489\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:18,723\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:18,963\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:19,244\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:19,535\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:19,793\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:20,073\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:20,311\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:20,601\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:20,846\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:21,076\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:21,312\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:21,564\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:21,808\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:22,070\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:22,354\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:22,594\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:22,823\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:23,051\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:23,292\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:23,609\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:23,893\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:24,135\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:24,451\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  40% 4/10 [01:00<01:41, 16.92s/it]2021-09-15 11:34:24,713\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:24,958\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:25,158\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect',\n",
      "                                                                   'office_network.map'],\n",
      "                                            'current_step': None,\n",
      "                                            'time': 6,\n",
      "                                            'ttc_remaining_on_current_step': np.ndarray((1, 7), dtype=int64, min=0.0, max=3.0, mean=0.857)},\n",
      "                                  'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222),\n",
      "                                  'prev_action': None,\n",
      "                                  'prev_reward': 0.0,\n",
      "                                  'rnn_state': [ np.ndarray((256,), dtype=float32, min=-0.364, max=0.363, mean=-0.002),\n",
      "                                                 np.ndarray((256,), dtype=float32, min=-0.661, max=0.763, mean=-0.007)]},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:34:25,166\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                      [ np.ndarray((1, 256), dtype=float32, min=-0.159, max=0.146, mean=-0.002),\n",
      "                        np.ndarray((1, 256), dtype=float32, min=-0.33, max=0.28, mean=-0.004)],\n",
      "                      { 'action_dist_inputs': np.ndarray((1, 3), dtype=float32, min=0.212, max=0.979, mean=0.61),\n",
      "                        'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "                        'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "                        'q_values': np.ndarray((1, 3), dtype=float32, min=0.212, max=0.979, mean=0.61)})}\n",
      "\n",
      "2021-09-15 11:34:25,168\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}\n",
      "2021-09-15 11:34:25,169\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [],\n",
      "                   'current_step': 'internet.connect',\n",
      "                   'time': 1,\n",
      "                   'ttc_remaining_on_current_step': 2}}}\n",
      "2021-09-15 11:34:25,171\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)\n",
      "2021-09-15 11:34:25,172\tINFO sampler.py:827 -- Filtered obs: np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)\n",
      "2021-09-15 11:34:25,190\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((5,), dtype=int64, min=0.0, max=1.0, mean=0.6),\n",
      "              'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((5,), dtype=float32, min=0.0, max=1.0, mean=0.2),\n",
      "              'eps_id': np.ndarray((5,), dtype=int64, min=622805473.0, max=622805473.0, mean=622805473.0),\n",
      "              'infos': np.ndarray((5,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 2, 'compromised_steps': [], 'compromised_flags': []}),\n",
      "              'obs': np.ndarray((5, 9), dtype=int64, min=0.0, max=1.0, mean=0.2),\n",
      "              'rewards': np.ndarray((5,), dtype=float32, min=0.0, max=2.0, mean=0.8),\n",
      "              'state_in_0': np.ndarray((1, 256), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'state_in_1': np.ndarray((1, 256), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'state_out_0': np.ndarray((5, 256), dtype=float32, min=-0.251, max=0.237, mean=-0.002),\n",
      "              'state_out_1': np.ndarray((5, 256), dtype=float32, min=-0.589, max=0.477, mean=-0.005),\n",
      "              'unroll_id': np.ndarray((5,), dtype=int64, min=337.0, max=337.0, mean=337.0),\n",
      "              'weights': np.ndarray((5,), dtype=float32, min=1.0, max=1.0, mean=1.0)}}\n",
      "\n",
      "2021-09-15 11:34:25,195\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((5,), dtype=int64, min=0.0, max=1.0, mean=0.6),\n",
      "  'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((5,), dtype=float32, min=0.0, max=1.0, mean=0.2),\n",
      "  'eps_id': np.ndarray((5,), dtype=int64, min=622805473.0, max=622805473.0, mean=622805473.0),\n",
      "  'infos': np.ndarray((5,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 2, 'compromised_steps': [], 'compromised_flags': []}),\n",
      "  'obs': np.ndarray((5, 9), dtype=int64, min=0.0, max=1.0, mean=0.2),\n",
      "  'rewards': np.ndarray((5,), dtype=float32, min=0.0, max=2.0, mean=0.8),\n",
      "  'seq_lens': np.ndarray((1,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "  'state_in_0': np.ndarray((1, 256), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'state_in_1': np.ndarray((1, 256), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'state_out_0': np.ndarray((5, 256), dtype=float32, min=-0.251, max=0.237, mean=-0.002),\n",
      "  'state_out_1': np.ndarray((5, 256), dtype=float32, min=-0.589, max=0.477, mean=-0.005),\n",
      "  'unroll_id': np.ndarray((5,), dtype=int64, min=337.0, max=337.0, mean=337.0),\n",
      "  'weights': np.ndarray((5,), dtype=float32, min=1.0, max=1.0, mean=1.0)}\n",
      "\n",
      "2021-09-15 11:34:25,197\tINFO replay_buffer.py:46 -- Estimated max memory usage for replay buffer is 0.09176 GB (20000.0 batches of size 5, 4588 bytes each), available system memory is 33.678143488 GB\n",
      "2021-09-15 11:34:25,204\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:25,422\tINFO rollout_worker.py:741 -- Generating sample batch of size 4\n",
      "2021-09-15 11:34:25,467\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:25,704\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:25,992\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:26,280\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:26,503\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:26,748\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:26,993\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:27,249\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:27,517\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:27,756\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:28,014\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:28,296\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:28,536\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:28,764\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:28,989\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:29,232\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:29,483\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:29,709\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:29,940\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:30,164\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:30,457\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:30,698\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:30,947\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:31,186\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:31,422\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:31,726\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:31,969\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:32,249\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:32,488\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:32,761\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:32,998\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:33,262\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:33,511\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:33,771\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:34,034\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:34,293\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:34,577\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:34,804\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:35,049\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:35,318\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:35,593\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:35,835\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:36,131\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:36,404\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:36,639\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:36,897\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:37,160\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:37,407\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:37,723\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:37,966\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:38,231\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:38,457\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:38,727\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:38,973\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:39,215\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:39,455\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:39,685\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:39,954\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:40,279\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:40,525\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:40,787\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:41,030\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:41,269\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:41,536\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:41,814\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:42,045\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:42,275\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:42,511\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:42,771\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:43,011\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:43,253\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:43,473\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:43,729\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:43,996\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:44,303\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  50% 5/10 [01:19<01:29, 17.98s/it]2021-09-15 11:34:44,602\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:44,870\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:45,095\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:45,342\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:45,605\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:45,845\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:46,071\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:46,293\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:46,540\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:46,772\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:47,019\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:47,287\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:47,574\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:47,837\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:48,078\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:48,328\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:48,588\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:48,841\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:49,104\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:49,368\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:49,638\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:49,875\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:50,136\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:50,409\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:50,640\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:50,870\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:51,104\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:51,365\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:51,596\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:51,832\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:52,068\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:52,293\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:52,555\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:52,791\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:53,043\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:53,289\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:53,518\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:53,760\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:54,079\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:54,377\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:54,601\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:54,847\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:55,114\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:55,385\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:55,634\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:55,892\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:56,157\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:56,385\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:56,648\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:56,875\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:57,182\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:57,455\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:57,695\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:57,943\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:58,211\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:58,511\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:58,774\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:59,020\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:59,258\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:59,488\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:34:59,776\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:00,018\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:00,245\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:00,514\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:00,796\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:01,101\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:01,327\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:01,591\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:01,865\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:02,118\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:02,403\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:02,779\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:03,015\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:03,259\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:03,502\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:03,774\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:04,029\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:04,275\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:04,511\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:04,775\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  60% 6/10 [01:40<01:15, 18.82s/it]2021-09-15 11:35:05,048\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:05,313\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:05,541\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:05,772\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:06,042\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:06,258\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:06,570\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:06,823\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:07,077\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:07,317\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:07,579\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:07,853\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:08,152\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:08,382\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:08,648\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:08,905\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:09,174\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:09,432\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:09,673\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:09,920\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:10,189\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:10,451\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:10,695\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:10,932\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:11,172\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:11,425\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:11,653\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:11,897\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:12,141\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:12,389\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:12,623\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:12,847\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:13,076\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:13,303\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:13,566\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:13,848\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:14,116\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:14,351\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:14,577\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:14,829\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:15,120\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:15,387\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:15,637\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:15,966\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:16,218\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:16,470\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:16,704\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:16,942\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:17,222\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:17,466\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:17,697\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:17,925\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:18,216\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:18,492\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:18,760\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:19,077\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:19,412\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:19,697\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:19,929\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:20,180\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:20,414\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:20,677\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:20,910\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:21,151\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:21,383\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:21,611\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:21,840\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:22,201\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:22,425\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:22,670\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:22,898\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:23,131\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:23,384\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:23,661\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:23,884\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:24,138\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:24,381\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  70% 7/10 [02:00<00:57, 19.08s/it]2021-09-15 11:35:24,660\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:24,940\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:25,176\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:25,424\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}\n",
      "2021-09-15 11:35:25,425\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 16,\n",
      "                   'ttc_remaining_on_current_step': 12}}}\n",
      "2021-09-15 11:35:25,426\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)\n",
      "2021-09-15 11:35:25,427\tINFO sampler.py:827 -- Filtered obs: np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)\n",
      "2021-09-15 11:35:25,430\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 16,\n",
      "                                            'ttc_remaining_on_current_step': 12},\n",
      "                                  'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222),\n",
      "                                  'prev_action': 1,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': [ np.ndarray((256,), dtype=float32, min=-0.275, max=0.258, mean=-0.007),\n",
      "                                                 np.ndarray((256,), dtype=float32, min=-0.626, max=0.599, mean=-0.018)]},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:35:25,438\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                      [ np.ndarray((1, 256), dtype=float32, min=-0.275, max=0.257, mean=-0.007),\n",
      "                        np.ndarray((1, 256), dtype=float32, min=-0.629, max=0.6, mean=-0.018)],\n",
      "                      { 'action_dist_inputs': np.ndarray((1, 3), dtype=float32, min=-0.051, max=-0.042, mean=-0.046),\n",
      "                        'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "                        'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "                        'q_values': np.ndarray((1, 3), dtype=float32, min=-0.051, max=-0.042, mean=-0.046)})}\n",
      "\n",
      "2021-09-15 11:35:25,472\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((28,), dtype=int64, min=0.0, max=2.0, mean=0.464),\n",
      "              'agent_index': np.ndarray((28,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((28,), dtype=float32, min=0.0, max=1.0, mean=0.036),\n",
      "              'eps_id': np.ndarray((28,), dtype=int64, min=259873321.0, max=259873321.0, mean=259873321.0),\n",
      "              'infos': np.ndarray((28,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'obs': np.ndarray((28, 9), dtype=int64, min=0.0, max=1.0, mean=0.266),\n",
      "              'rewards': np.ndarray((28,), dtype=float32, min=0.0, max=2.0, mean=0.429),\n",
      "              'state_in_0': np.ndarray((2, 256), dtype=float32, min=-0.275, max=0.257, mean=-0.004),\n",
      "              'state_in_1': np.ndarray((2, 256), dtype=float32, min=-0.633, max=0.601, mean=-0.009),\n",
      "              'state_out_0': np.ndarray((28, 256), dtype=float32, min=-0.31, max=0.353, mean=-0.006),\n",
      "              'state_out_1': np.ndarray((28, 256), dtype=float32, min=-0.634, max=0.753, mean=-0.015),\n",
      "              'unroll_id': np.ndarray((28,), dtype=int64, min=589.0, max=589.0, mean=589.0),\n",
      "              'weights': np.ndarray((28,), dtype=float32, min=1.0, max=1.0, mean=1.0)}}\n",
      "\n",
      "2021-09-15 11:35:25,477\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((28,), dtype=int64, min=0.0, max=2.0, mean=0.464),\n",
      "  'agent_index': np.ndarray((28,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((28,), dtype=float32, min=0.0, max=1.0, mean=0.036),\n",
      "  'eps_id': np.ndarray((28,), dtype=int64, min=259873321.0, max=259873321.0, mean=259873321.0),\n",
      "  'infos': np.ndarray((28,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'obs': np.ndarray((28, 9), dtype=int64, min=0.0, max=1.0, mean=0.266),\n",
      "  'rewards': np.ndarray((28,), dtype=float32, min=0.0, max=2.0, mean=0.429),\n",
      "  'seq_lens': np.ndarray((2,), dtype=int32, min=8.0, max=20.0, mean=14.0),\n",
      "  'state_in_0': np.ndarray((2, 256), dtype=float32, min=-0.275, max=0.257, mean=-0.004),\n",
      "  'state_in_1': np.ndarray((2, 256), dtype=float32, min=-0.633, max=0.601, mean=-0.009),\n",
      "  'state_out_0': np.ndarray((28, 256), dtype=float32, min=-0.31, max=0.353, mean=-0.006),\n",
      "  'state_out_1': np.ndarray((28, 256), dtype=float32, min=-0.634, max=0.753, mean=-0.015),\n",
      "  'unroll_id': np.ndarray((28,), dtype=int64, min=589.0, max=589.0, mean=589.0),\n",
      "  'weights': np.ndarray((28,), dtype=float32, min=1.0, max=1.0, mean=1.0)}\n",
      "\n",
      "2021-09-15 11:35:25,479\tINFO replay_buffer.py:46 -- Estimated max memory usage for replay buffer is 0.02266 GB (5000.0 batches of size 20, 4532 bytes each), available system memory is 33.678143488 GB\n",
      "2021-09-15 11:35:25,488\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:25,684\tINFO rollout_worker.py:741 -- Generating sample batch of size 4\n",
      "2021-09-15 11:35:25,727\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:25,977\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:26,380\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:26,628\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:26,903\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:27,164\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:27,419\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:27,679\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:27,941\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:28,171\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:28,435\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:28,673\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:28,921\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:29,189\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:29,441\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:29,683\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:29,941\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:30,171\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:30,397\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:30,634\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:30,877\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:31,158\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:31,381\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:31,633\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:31,858\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:32,103\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:32,362\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:32,591\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:32,843\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:33,084\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:33,359\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:33,602\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:33,860\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:34,122\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:34,348\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:34,573\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:34,851\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:35,077\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:35,334\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:35,623\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:35,885\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:36,140\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:36,378\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:36,647\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:36,899\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:37,212\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:37,435\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:37,704\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:37,942\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:38,215\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:38,467\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:38,709\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:38,950\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:39,221\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:39,466\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:39,720\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:39,997\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:40,257\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:40,503\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:40,740\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:40,973\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:41,216\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:41,461\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:41,700\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:42,020\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:42,283\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:42,537\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:42,838\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:43,112\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  80% 8/10 [02:18<00:37, 18.97s/it]2021-09-15 11:35:43,428\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:43,704\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:43,992\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:44,268\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:44,571\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:44,816\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:45,113\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:45,377\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:45,662\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:45,881\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:46,133\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:46,377\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:46,654\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:46,917\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:47,149\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:47,401\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:47,628\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:47,860\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:48,076\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:48,312\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:48,549\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:48,792\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:49,126\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:49,406\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:49,634\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:49,910\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:50,162\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:50,433\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:50,693\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:50,980\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:51,246\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:51,488\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:51,738\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:51,989\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:52,245\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:52,503\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:52,757\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:52,993\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:53,285\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:53,511\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:53,784\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:54,130\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:54,354\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:54,627\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:54,859\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:55,113\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:55,382\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:55,653\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:55,891\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:56,165\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:56,396\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:56,652\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:56,899\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:57,151\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:57,405\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:57,641\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:57,885\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:58,180\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:58,440\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:58,744\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:58,984\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:59,226\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:59,485\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:35:59,767\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:00,003\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:00,263\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:00,729\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0:  90% 9/10 [02:36<00:18, 18.55s/it]2021-09-15 11:36:01,079\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:01,368\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:01,674\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:01,893\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:02,169\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:02,455\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:02,774\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:03,012\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:03,276\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:03,590\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:03,866\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:04,227\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:04,456\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:04,694\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:04,936\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:05,171\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:05,435\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:05,758\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:06,245\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:06,574\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:06,904\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:07,198\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:07,480\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:07,752\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:08,092\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:08,368\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:08,938\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:09,315\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:09,610\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:10,011\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:10,394\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "2021-09-15 11:36:10,680\tDEBUG train_ops.py:183 -- == sgd epochs for default_policy ==\n",
      "R2D2_tiny_0: 100% 10/10 [02:46<00:00, 16.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #0: reward: 16.0\n",
      "Episode #1: reward: -9990.0\n",
      "Episode #2: reward: -16840.0\n",
      "Episode #3: reward: 46.0\n",
      "Episode #4: reward: 94.0\n",
      "Episode #5: reward: 133.0\n",
      "Episode #6: reward: -7611.0\n",
      "Episode #7: reward: -2712.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:36:12,231\tWARNING deprecation.py:38 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2021-09-15 11:36:12,240\tDEBUG rollout_worker.py:1357 -- Creating policy for default_policy\n",
      "2021-09-15 11:36:12,242\tDEBUG preprocessors.py:249 -- Creating sub-preprocessor for Box(0.0, 1.0, (3,), float32)\n",
      "2021-09-15 11:36:12,244\tDEBUG preprocessors.py:249 -- Creating sub-preprocessor for Box(0, 1, (9,), int8)\n",
      "2021-09-15 11:36:12,245\tDEBUG catalog.py:707 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x7f5c542d45b0>: Dict(action_mask:Box(0.0, 1.0, (3,), float32), obs:Box(0, 1, (9,), int8)) -> (12,)\n",
      "2021-09-15 11:36:12,247\tINFO catalog.py:412 -- Wrapping <class 'ray.rllib.contrib.alpha_zero.models.custom_torch_models.DenseModel'> as None\n",
      "2021-09-15 11:36:12,249\tDEBUG preprocessors.py:249 -- Creating sub-preprocessor for Box(0.0, 1.0, (3,), float32)\n",
      "2021-09-15 11:36:12,250\tDEBUG preprocessors.py:249 -- Creating sub-preprocessor for Box(0, 1, (9,), int8)\n",
      "2021-09-15 11:36:12,255\tINFO torch_policy.py:145 -- TorchPolicy (worker=local) running on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #8: reward: 133.0\n",
      "Episode #9: reward: 97.0\n",
      "<class 'ray.rllib.agents.trainer_template.AlphaZero'> {'framework': 'torch', 'model': {'custom_model': 'alpha_zero_dense_model'}, 'env': <class '__main__.AlphaZeroWrapper'>, 'env_config': {'attack_graph': <attack_simulator.graph.AttackGraph object at 0x7f5c544d7850>, 'env_class': <class 'attack_simulator.env.AttackSimulationEnv'>}, 'seed': 0, 'create_env_on_driver': True, 'num_workers': 0, 'batch_mode': 'complete_episodes', 'log_level': 'DEBUG'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:36:12,495\tINFO rollout_worker.py:1379 -- Built policy map: {}\n",
      "2021-09-15 11:36:12,496\tINFO rollout_worker.py:1380 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x7f5c542d45b0>}\n",
      "2021-09-15 11:36:12,497\tINFO rollout_worker.py:611 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f5b3457fe50>}\n",
      "2021-09-15 11:36:12,499\tDEBUG rollout_worker.py:716 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f5b34483ac0> (<AlphaZeroWrapper instance>), policies {}\n",
      "2021-09-15 11:36:12,501\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "AlphaZero_tiny_0:   0% 0/10 [00:00<?, ?it/s]2021-09-15 11:36:12,505\tDEBUG sampler.py:538 -- No episode horizon specified, assuming inf.\n",
      "2021-09-15 11:36:12,514\tDEBUG preprocessors.py:249 -- Creating sub-preprocessor for Box(0.0, 1.0, (3,), float32)\n",
      "2021-09-15 11:36:12,515\tDEBUG preprocessors.py:249 -- Creating sub-preprocessor for Box(0, 1, (9,), int8)\n",
      "2021-09-15 11:37:12,517\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.333)}}}\n",
      "2021-09-15 11:37:12,519\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 3,\n",
      "                   'ttc_remaining_on_current_step': 11}}}\n",
      "2021-09-15 11:37:12,522\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.5)\n",
      "2021-09-15 11:37:12,523\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.5)\n",
      "2021-09-15 11:37:12,525\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 3,\n",
      "                                            'ttc_remaining_on_current_step': 11},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "                                  'prev_action': 0,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:37:12,718\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:37:13,888\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((14,), dtype=int64, min=0.0, max=2.0, mean=1.143),\n",
      "              'agent_index': np.ndarray((14,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((14,), dtype=float32, min=0.0, max=1.0, mean=0.071),\n",
      "              'eps_id': np.ndarray((14,), dtype=int64, min=1930240526.0, max=1930240526.0, mean=1930240526.0),\n",
      "              'infos': np.ndarray((14,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((14, 3), dtype=float32, min=0.002, max=0.991, mean=0.333),\n",
      "              'new_obs': np.ndarray((14, 12), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "              'obs': np.ndarray((14, 12), dtype=float32, min=0.0, max=1.0, mean=0.488),\n",
      "              'prev_actions': np.ndarray((14,), dtype=int64, min=0.0, max=2.0, mean=1.143),\n",
      "              'prev_rewards': np.ndarray((14,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((14,), dtype=float32, min=0.0, max=14.0, mean=1.0),\n",
      "              't': np.ndarray((14,), dtype=int64, min=0.0, max=13.0, mean=6.5),\n",
      "              'unroll_id': np.ndarray((14,), dtype=int64, min=807.0, max=807.0, mean=807.0),\n",
      "              'value_label': np.ndarray((14,), dtype=float64, min=1.0, max=1.0, mean=1.0)}}\n",
      "\n",
      "2021-09-15 11:37:28,085\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((201,), dtype=int64, min=0.0, max=2.0, mean=1.015),\n",
      "  'agent_index': np.ndarray((201,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((201,), dtype=float32, min=0.0, max=1.0, mean=0.1),\n",
      "  'eps_id': np.ndarray((201,), dtype=int64, min=158360665.0, max=1937386629.0, mean=1141712206.488),\n",
      "  'infos': np.ndarray((201,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((201, 3), dtype=float32, min=0.002, max=0.994, mean=0.333),\n",
      "  'new_obs': np.ndarray((201, 12), dtype=float32, min=0.0, max=1.0, mean=0.456),\n",
      "  'obs': np.ndarray((201, 12), dtype=float32, min=0.0, max=1.0, mean=0.446),\n",
      "  'prev_actions': np.ndarray((201,), dtype=int64, min=0.0, max=2.0, mean=0.905),\n",
      "  'prev_rewards': np.ndarray((201,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((201,), dtype=float32, min=0.0, max=14.0, mean=0.517),\n",
      "  't': np.ndarray((201,), dtype=int64, min=0.0, max=19.0, mean=5.821),\n",
      "  'unroll_id': np.ndarray((201,), dtype=int64, min=799.0, max=818.0, mean=809.204),\n",
      "  'value_label': np.ndarray((201,), dtype=float64, min=-1.0, max=1.0, mean=0.124)}\n",
      "\n",
      "2021-09-15 11:37:28,086\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:38:28,125\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}}\n",
      "2021-09-15 11:38:28,126\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 8,\n",
      "                   'ttc_remaining_on_current_step': 5}}}\n",
      "2021-09-15 11:38:28,128\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:38:28,129\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:38:28,131\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 8,\n",
      "                                            'ttc_remaining_on_current_step': 5},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "                                  'prev_action': 0,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:38:28,305\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:38:28,646\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((13,), dtype=int64, min=0.0, max=2.0, mean=1.077),\n",
      "              'agent_index': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((13,), dtype=float32, min=0.0, max=1.0, mean=0.077),\n",
      "              'eps_id': np.ndarray((13,), dtype=int64, min=1511060853.0, max=1511060853.0, mean=1511060853.0),\n",
      "              'infos': np.ndarray((13,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((13, 3), dtype=float32, min=0.15, max=0.628, mean=0.333),\n",
      "              'new_obs': np.ndarray((13, 12), dtype=float32, min=0.0, max=1.0, mean=0.41),\n",
      "              'obs': np.ndarray((13, 12), dtype=float32, min=0.0, max=1.0, mean=0.404),\n",
      "              'prev_actions': np.ndarray((13,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "              'prev_rewards': np.ndarray((13,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((13,), dtype=float32, min=0.0, max=2.0, mean=0.154),\n",
      "              't': np.ndarray((13,), dtype=int64, min=0.0, max=12.0, mean=6.0),\n",
      "              'unroll_id': np.ndarray((13,), dtype=int64, min=851.0, max=851.0, mean=851.0),\n",
      "              'value_label': np.ndarray((13,), dtype=float64, min=-1.0, max=-1.0, mean=-1.0)}}\n",
      "\n",
      "2021-09-15 11:38:47,662\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((205,), dtype=int64, min=0.0, max=2.0, mean=1.049),\n",
      "  'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((205,), dtype=float32, min=0.0, max=1.0, mean=0.117),\n",
      "  'eps_id': np.ndarray((205,), dtype=int64, min=135257702.0, max=1969283239.0, mean=880272560.249),\n",
      "  'infos': np.ndarray((205,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((205, 3), dtype=float32, min=0.002, max=0.994, mean=0.333),\n",
      "  'new_obs': np.ndarray((205, 12), dtype=float32, min=0.0, max=1.0, mean=0.454),\n",
      "  'obs': np.ndarray((205, 12), dtype=float32, min=0.0, max=1.0, mean=0.441),\n",
      "  'prev_actions': np.ndarray((205,), dtype=int64, min=0.0, max=2.0, mean=0.922),\n",
      "  'prev_rewards': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((205,), dtype=float32, min=0.0, max=12.0, mean=0.493),\n",
      "  't': np.ndarray((205,), dtype=int64, min=0.0, max=26.0, mean=5.498),\n",
      "  'unroll_id': np.ndarray((205,), dtype=int64, min=848.0, max=871.0, mean=861.176),\n",
      "  'value_label': np.ndarray((205,), dtype=float64, min=-1.0, max=1.0, mean=0.102)}\n",
      "\n",
      "2021-09-15 11:38:47,664\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:39:47,907\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.333)}}}\n",
      "2021-09-15 11:39:47,909\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect'],\n",
      "                   'current_step': 'internet.connect',\n",
      "                   'time': 1,\n",
      "                   'ttc_remaining_on_current_step': 0}}}\n",
      "2021-09-15 11:39:47,914\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.5)\n",
      "2021-09-15 11:39:47,915\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.5)\n",
      "2021-09-15 11:39:47,917\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect'],\n",
      "                                            'current_step': 'internet.connect',\n",
      "                                            'time': 1,\n",
      "                                            'ttc_remaining_on_current_step': 0},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "                                  'prev_action': 0,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:39:48,081\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:39:52,729\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((31,), dtype=int64, min=0.0, max=2.0, mean=1.129),\n",
      "              'agent_index': np.ndarray((31,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((31,), dtype=float32, min=0.0, max=1.0, mean=0.032),\n",
      "              'eps_id': np.ndarray((31,), dtype=int64, min=1457548605.0, max=1457548605.0, mean=1457548605.0),\n",
      "              'infos': np.ndarray((31,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((31, 3), dtype=float32, min=0.122, max=0.657, mean=0.333),\n",
      "              'new_obs': np.ndarray((31, 12), dtype=float32, min=0.0, max=1.0, mean=0.427),\n",
      "              'obs': np.ndarray((31, 12), dtype=float32, min=0.0, max=1.0, mean=0.425),\n",
      "              'prev_actions': np.ndarray((31,), dtype=int64, min=0.0, max=2.0, mean=1.097),\n",
      "              'prev_rewards': np.ndarray((31,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((31,), dtype=float32, min=0.0, max=4.0, mean=0.129),\n",
      "              't': np.ndarray((31,), dtype=int64, min=0.0, max=30.0, mean=15.0),\n",
      "              'unroll_id': np.ndarray((31,), dtype=int64, min=907.0, max=907.0, mean=907.0),\n",
      "              'value_label': np.ndarray((31,), dtype=float64, min=-1.0, max=-1.0, mean=-1.0)}}\n",
      "\n",
      "2021-09-15 11:40:12,153\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((211,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "  'agent_index': np.ndarray((211,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((211,), dtype=float32, min=0.0, max=1.0, mean=0.095),\n",
      "  'eps_id': np.ndarray((211,), dtype=int64, min=82982131.0, max=1982945636.0, mean=1228158052.038),\n",
      "  'infos': np.ndarray((211,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 1, 'compromised_steps': [], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((211, 3), dtype=float32, min=0.001, max=0.994, mean=0.333),\n",
      "  'new_obs': np.ndarray((211, 12), dtype=float32, min=0.0, max=1.0, mean=0.446),\n",
      "  'obs': np.ndarray((211, 12), dtype=float32, min=0.0, max=1.0, mean=0.436),\n",
      "  'prev_actions': np.ndarray((211,), dtype=int64, min=0.0, max=2.0, mean=0.867),\n",
      "  'prev_rewards': np.ndarray((211,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((211,), dtype=float32, min=0.0, max=14.0, mean=0.398),\n",
      "  't': np.ndarray((211,), dtype=int64, min=0.0, max=30.0, mean=7.474),\n",
      "  'unroll_id': np.ndarray((211,), dtype=int64, min=906.0, max=925.0, mean=915.469),\n",
      "  'value_label': np.ndarray((211,), dtype=float64, min=-1.0, max=1.0, mean=-0.299)}\n",
      "\n",
      "2021-09-15 11:40:12,155\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:41:12,279\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}}\n",
      "2021-09-15 11:41:12,281\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 12,\n",
      "                   'ttc_remaining_on_current_step': 12}}}\n",
      "2021-09-15 11:41:12,282\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:41:12,283\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:41:12,285\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 12,\n",
      "                                            'ttc_remaining_on_current_step': 12},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "                                  'prev_action': 0,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:41:12,458\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:41:14,211\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((24,), dtype=int64, min=0.0, max=2.0, mean=1.042),\n",
      "              'agent_index': np.ndarray((24,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((24,), dtype=float32, min=0.0, max=1.0, mean=0.042),\n",
      "              'eps_id': np.ndarray((24,), dtype=int64, min=97139416.0, max=97139416.0, mean=97139416.0),\n",
      "              'infos': np.ndarray((24,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((24, 3), dtype=float32, min=0.081, max=0.654, mean=0.333),\n",
      "              'new_obs': np.ndarray((24, 12), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "              'obs': np.ndarray((24, 12), dtype=float32, min=0.0, max=1.0, mean=0.413),\n",
      "              'prev_actions': np.ndarray((24,), dtype=int64, min=0.0, max=2.0, mean=1.042),\n",
      "              'prev_rewards': np.ndarray((24,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((24,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              't': np.ndarray((24,), dtype=int64, min=0.0, max=23.0, mean=11.5),\n",
      "              'unroll_id': np.ndarray((24,), dtype=int64, min=977.0, max=977.0, mean=977.0),\n",
      "              'value_label': np.ndarray((24,), dtype=float64, min=-1.0, max=-1.0, mean=-1.0)}}\n",
      "\n",
      "2021-09-15 11:41:28,288\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((204,), dtype=int64, min=0.0, max=2.0, mean=0.975),\n",
      "  'agent_index': np.ndarray((204,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((204,), dtype=float32, min=0.0, max=1.0, mean=0.098),\n",
      "  'eps_id': np.ndarray((204,), dtype=int64, min=57330932.0, max=1954244505.0, mean=933092511.804),\n",
      "  'infos': np.ndarray((204,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((204, 3), dtype=float32, min=0.002, max=0.997, mean=0.333),\n",
      "  'new_obs': np.ndarray((204, 12), dtype=float32, min=0.0, max=1.0, mean=0.446),\n",
      "  'obs': np.ndarray((204, 12), dtype=float32, min=0.0, max=1.0, mean=0.436),\n",
      "  'prev_actions': np.ndarray((204,), dtype=int64, min=0.0, max=2.0, mean=0.897),\n",
      "  'prev_rewards': np.ndarray((204,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((204,), dtype=float32, min=0.0, max=8.0, mean=0.377),\n",
      "  't': np.ndarray((204,), dtype=int64, min=0.0, max=27.0, mean=7.039),\n",
      "  'unroll_id': np.ndarray((204,), dtype=int64, min=967.0, max=986.0, mean=977.172),\n",
      "  'value_label': np.ndarray((204,), dtype=float64, min=-1.0, max=1.0, mean=-0.265)}\n",
      "\n",
      "2021-09-15 11:41:28,290\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:42:28,402\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}}\n",
      "2021-09-15 11:42:28,404\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 41,\n",
      "                   'ttc_remaining_on_current_step': 11}}}\n",
      "2021-09-15 11:42:28,406\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:42:28,408\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:42:28,410\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 41,\n",
      "                                            'ttc_remaining_on_current_step': 11},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "                                  'prev_action': 0,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:42:28,605\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:42:29,630\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((52,), dtype=int64, min=0.0, max=2.0, mean=0.885),\n",
      "              'agent_index': np.ndarray((52,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((52,), dtype=float32, min=0.0, max=1.0, mean=0.019),\n",
      "              'eps_id': np.ndarray((52,), dtype=int64, min=936450292.0, max=936450292.0, mean=936450292.0),\n",
      "              'infos': np.ndarray((52,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((52, 3), dtype=float32, min=0.003, max=0.994, mean=0.333),\n",
      "              'new_obs': np.ndarray((52, 12), dtype=float32, min=0.0, max=1.0, mean=0.433),\n",
      "              'obs': np.ndarray((52, 12), dtype=float32, min=0.0, max=1.0, mean=0.431),\n",
      "              'prev_actions': np.ndarray((52,), dtype=int64, min=0.0, max=2.0, mean=0.885),\n",
      "              'prev_rewards': np.ndarray((52,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((52,), dtype=float32, min=0.0, max=10.0, mean=0.192),\n",
      "              't': np.ndarray((52,), dtype=int64, min=0.0, max=51.0, mean=25.5),\n",
      "              'unroll_id': np.ndarray((52,), dtype=int64, min=1025.0, max=1025.0, mean=1025.0),\n",
      "              'value_label': np.ndarray((52,), dtype=float64, min=1.0, max=1.0, mean=1.0)}}\n",
      "\n",
      "2021-09-15 11:42:51,912\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((212,), dtype=int64, min=0.0, max=2.0, mean=0.92),\n",
      "  'agent_index': np.ndarray((212,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((212,), dtype=float32, min=0.0, max=1.0, mean=0.061),\n",
      "  'eps_id': np.ndarray((212,), dtype=int64, min=84565125.0, max=1501784960.0, mean=696750193.311),\n",
      "  'infos': np.ndarray((212,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((212, 3), dtype=float32, min=0.002, max=0.994, mean=0.333),\n",
      "  'new_obs': np.ndarray((212, 12), dtype=float32, min=0.0, max=1.0, mean=0.443),\n",
      "  'obs': np.ndarray((212, 12), dtype=float32, min=0.0, max=1.0, mean=0.438),\n",
      "  'prev_actions': np.ndarray((212,), dtype=int64, min=0.0, max=2.0, mean=0.868),\n",
      "  'prev_rewards': np.ndarray((212,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((212,), dtype=float32, min=0.0, max=10.0, mean=0.34),\n",
      "  't': np.ndarray((212,), dtype=int64, min=0.0, max=51.0, mean=15.231),\n",
      "  'unroll_id': np.ndarray((212,), dtype=int64, min=1025.0, max=1037.0, mean=1030.887),\n",
      "  'value_label': np.ndarray((212,), dtype=float64, min=-1.0, max=1.0, mean=0.5)}\n",
      "\n",
      "2021-09-15 11:42:51,913\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:43:51,915\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}}\n",
      "2021-09-15 11:43:51,917\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 7,\n",
      "                   'ttc_remaining_on_current_step': 8}}}\n",
      "2021-09-15 11:43:51,920\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:43:51,922\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:43:51,924\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 7,\n",
      "                                            'ttc_remaining_on_current_step': 8},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "                                  'prev_action': 2,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:43:52,096\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:43:52,906\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((15,), dtype=int64, min=0.0, max=2.0, mean=1.133),\n",
      "              'agent_index': np.ndarray((15,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((15,), dtype=float32, min=0.0, max=1.0, mean=0.067),\n",
      "              'eps_id': np.ndarray((15,), dtype=int64, min=1222160302.0, max=1222160302.0, mean=1222160302.0),\n",
      "              'infos': np.ndarray((15,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((15, 3), dtype=float32, min=0.131, max=0.653, mean=0.333),\n",
      "              'new_obs': np.ndarray((15, 12), dtype=float32, min=0.0, max=1.0, mean=0.411),\n",
      "              'obs': np.ndarray((15, 12), dtype=float32, min=0.0, max=1.0, mean=0.406),\n",
      "              'prev_actions': np.ndarray((15,), dtype=int64, min=0.0, max=2.0, mean=1.067),\n",
      "              'prev_rewards': np.ndarray((15,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((15,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              't': np.ndarray((15,), dtype=int64, min=0.0, max=14.0, mean=7.0),\n",
      "              'unroll_id': np.ndarray((15,), dtype=int64, min=1070.0, max=1070.0, mean=1070.0),\n",
      "              'value_label': np.ndarray((15,), dtype=float64, min=-1.0, max=-1.0, mean=-1.0)}}\n",
      "\n",
      "2021-09-15 11:44:18,493\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((208,), dtype=int64, min=0.0, max=2.0, mean=1.048),\n",
      "  'agent_index': np.ndarray((208,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((208,), dtype=float32, min=0.0, max=1.0, mean=0.072),\n",
      "  'eps_id': np.ndarray((208,), dtype=int64, min=26706789.0, max=1970340143.0, mean=1093728376.62),\n",
      "  'infos': np.ndarray((208,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((208, 3), dtype=float32, min=0.002, max=0.996, mean=0.333),\n",
      "  'new_obs': np.ndarray((208, 12), dtype=float32, min=0.0, max=1.0, mean=0.45),\n",
      "  'obs': np.ndarray((208, 12), dtype=float32, min=0.0, max=1.0, mean=0.443),\n",
      "  'prev_actions': np.ndarray((208,), dtype=int64, min=0.0, max=2.0, mean=1.005),\n",
      "  'prev_rewards': np.ndarray((208,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((208,), dtype=float32, min=0.0, max=13.0, mean=0.447),\n",
      "  't': np.ndarray((208,), dtype=int64, min=0.0, max=30.0, mean=9.423),\n",
      "  'unroll_id': np.ndarray((208,), dtype=int64, min=1070.0, max=1084.0, mean=1077.149),\n",
      "  'value_label': np.ndarray((208,), dtype=float64, min=-1.0, max=1.0, mean=0.308)}\n",
      "\n",
      "2021-09-15 11:44:18,494\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:45:18,586\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}}\n",
      "2021-09-15 11:45:18,587\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 9,\n",
      "                   'ttc_remaining_on_current_step': 18}}}\n",
      "2021-09-15 11:45:18,588\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:45:18,589\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:45:18,591\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 9,\n",
      "                                            'ttc_remaining_on_current_step': 18},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "                                  'prev_action': 2,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:45:18,782\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n",
      "2021-09-15 11:45:21,788\tINFO simple_list_collector.py:657 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'actions': np.ndarray((27,), dtype=int64, min=0.0, max=2.0, mean=1.296),\n",
      "              'agent_index': np.ndarray((27,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "              'dones': np.ndarray((27,), dtype=float32, min=0.0, max=1.0, mean=0.037),\n",
      "              'eps_id': np.ndarray((27,), dtype=int64, min=1276844228.0, max=1276844228.0, mean=1276844228.0),\n",
      "              'infos': np.ndarray((27,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "              'mcts_policies': np.ndarray((27, 3), dtype=float32, min=0.079, max=0.696, mean=0.333),\n",
      "              'new_obs': np.ndarray((27, 12), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "              'obs': np.ndarray((27, 12), dtype=float32, min=0.0, max=1.0, mean=0.414),\n",
      "              'prev_actions': np.ndarray((27,), dtype=int64, min=0.0, max=2.0, mean=1.222),\n",
      "              'prev_rewards': np.ndarray((27,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              'rewards': np.ndarray((27,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "              't': np.ndarray((27,), dtype=int64, min=0.0, max=26.0, mean=13.0),\n",
      "              'unroll_id': np.ndarray((27,), dtype=int64, min=1117.0, max=1117.0, mean=1117.0),\n",
      "              'value_label': np.ndarray((27,), dtype=float64, min=-1.0, max=-1.0, mean=-1.0)}}\n",
      "\n",
      "2021-09-15 11:45:21,794\tINFO rollout_worker.py:779 -- Completed sample batch:\n",
      "\n",
      "{ 'actions': np.ndarray((218,), dtype=int64, min=0.0, max=2.0, mean=0.954),\n",
      "  'agent_index': np.ndarray((218,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "  'dones': np.ndarray((218,), dtype=float32, min=0.0, max=1.0, mean=0.06),\n",
      "  'eps_id': np.ndarray((218,), dtype=int64, min=3273239.0, max=1833466522.0, mean=1006034700.073),\n",
      "  'infos': np.ndarray((218,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "  'mcts_policies': np.ndarray((218, 3), dtype=float32, min=0.001, max=0.995, mean=0.333),\n",
      "  'new_obs': np.ndarray((218, 12), dtype=float32, min=0.0, max=1.0, mean=0.43),\n",
      "  'obs': np.ndarray((218, 12), dtype=float32, min=0.0, max=1.0, mean=0.425),\n",
      "  'prev_actions': np.ndarray((218,), dtype=int64, min=0.0, max=2.0, mean=0.904),\n",
      "  'prev_rewards': np.ndarray((218,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "  'rewards': np.ndarray((218,), dtype=float32, min=0.0, max=10.0, mean=0.216),\n",
      "  't': np.ndarray((218,), dtype=int64, min=0.0, max=33.0, mean=10.931),\n",
      "  'unroll_id': np.ndarray((218,), dtype=int64, min=1105.0, max=1117.0, mean=1111.734),\n",
      "  'value_label': np.ndarray((218,), dtype=float64, min=-1.0, max=1.0, mean=-0.505)}\n",
      "\n",
      "2021-09-15 11:45:21,802\tINFO rollout_worker.py:924 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'count': 4142,\n",
      "  'policy_batches': { 'default_policy': { 'actions': np.ndarray((4142,), dtype=int64, min=0.0, max=2.0, mean=0.981),\n",
      "                                          'agent_index': np.ndarray((4142,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                                          'dones': np.ndarray((4142,), dtype=float32, min=0.0, max=1.0, mean=0.085),\n",
      "                                          'eps_id': np.ndarray((4142,), dtype=int64, min=2454178.0, max=1982945636.0, mean=1036098339.637),\n",
      "                                          'infos': np.ndarray((4142,), dtype=object, head={'time': 1, 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=0.0, mean=0.0), 'current_step': 'internet.connect', 'ttc_remaining_on_current_step': 0, 'compromised_steps': ['internet.connect'], 'compromised_flags': []}),\n",
      "                                          'mcts_policies': np.ndarray((4142, 3), dtype=float32, min=0.001, max=0.997, mean=0.333),\n",
      "                                          'new_obs': np.ndarray((4142, 12), dtype=float32, min=0.0, max=1.0, mean=0.445),\n",
      "                                          'obs': np.ndarray((4142, 12), dtype=float32, min=0.0, max=1.0, mean=0.436),\n",
      "                                          'prev_actions': np.ndarray((4142,), dtype=int64, min=0.0, max=2.0, mean=0.889),\n",
      "                                          'prev_rewards': np.ndarray((4142,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "                                          'rewards': np.ndarray((4142,), dtype=float32, min=0.0, max=20.0, mean=0.39),\n",
      "                                          't': np.ndarray((4142,), dtype=int64, min=0.0, max=61.0, mean=9.381),\n",
      "                                          'unroll_id': np.ndarray((4142,), dtype=int64, min=767.0, max=1117.0, mean=945.97),\n",
      "                                          'value_label': np.ndarray((4142,), dtype=float64, min=-1.0, max=1.0, mean=-0.056)}},\n",
      "  'type': 'MultiAgentBatch'}\n",
      "\n",
      "2021-09-15 11:45:21,847\tDEBUG rollout_worker.py:953 -- Training out:\n",
      "\n",
      "{ 'default_policy': { 'learner_stats': { 'policy_loss': np.ndarray((), dtype=float32, min=1.082, max=1.082, mean=1.082),\n",
      "                                         'total_loss': np.ndarray((), dtype=float32, min=1.044, max=1.044, mean=1.044),\n",
      "                                         'value_loss': np.ndarray((), dtype=float32, min=1.005, max=1.005, mean=1.005)}}}\n",
      "\n",
      "AlphaZero_tiny_0:  10% 1/10 [09:10<1:22:35, 550.64s/it]2021-09-15 11:45:23,144\tINFO rollout_worker.py:741 -- Generating sample batch of size 200\n",
      "2021-09-15 11:46:23,267\tINFO sampler.py:592 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((3,), dtype=int8, min=1.0, max=1.0, mean=1.0),\n",
      "                   'obs': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.222)}}}\n",
      "2021-09-15 11:46:23,269\tINFO sampler.py:594 -- Info return from env: { 0: { 'agent0': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                   'compromised_flags': [],\n",
      "                   'compromised_steps': [ 'internet.connect',\n",
      "                                          'office_network.connect'],\n",
      "                   'current_step': 'office_network.map',\n",
      "                   'time': 9,\n",
      "                   'ttc_remaining_on_current_step': 28}}}\n",
      "2021-09-15 11:46:23,271\tINFO sampler.py:823 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:46:23,272\tINFO sampler.py:827 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417)\n",
      "2021-09-15 11:46:23,274\tINFO sampler.py:1013 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': { 'attack_surface': np.ndarray((7,), dtype=int64, min=0.0, max=1.0, mean=0.143),\n",
      "                                            'compromised_flags': [ ],\n",
      "                                            'compromised_steps': [ 'internet.connect',\n",
      "                                                                   'office_network.connect'],\n",
      "                                            'current_step': 'office_network.map',\n",
      "                                            'time': 9,\n",
      "                                            'ttc_remaining_on_current_step': 28},\n",
      "                                  'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.417),\n",
      "                                  'prev_action': 2,\n",
      "                                  'prev_reward': 0,\n",
      "                                  'rnn_state': []},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-09-15 11:46:23,709\tINFO sampler.py:1034 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "                      [],\n",
      "                      {})}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%capture noise --no-stderr\n",
    "\n",
    "savename = 'data.csv'\n",
    "\n",
    "df = generate(savename) if not os.path.exists(savename) else pd.read_csv(savename, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf8dc-705f-4678-a445-9d54edb194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984bbe-82c6-4dca-8436-6d78cf97d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid', rc={'figure.figsize': (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5424a47-3486-472b-a604-779d30ca7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x='Graph size', y='Returns', hue='Agent', ci='sd')\n",
    "g.legend(title='Agent', loc='upper left')\n",
    "g.set_title('Returns vs Size (random attacker)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2d787-49db-4d90-a8ae-a69d97c085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x='Graph size', y='Episode lengths', hue='Agent', ci='sd')\n",
    "g.legend(title='Agent', loc='upper left')\n",
    "g.set_title('Episode lengths vs Size (random attacker)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6d9d1-2b14-4e48-a4a9-85785a112121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 32)\n",
    "df.groupby('Agent').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd9fae-6a2a-4748-b36c-c445d9fcc012",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = AttackSimulationEnv({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3d771-f993-41f6-bf68-809f9ec0fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9b4e1-4d1c-4d64-9519-27239c51ed52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
