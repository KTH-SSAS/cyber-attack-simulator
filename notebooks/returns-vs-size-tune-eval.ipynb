{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda8002-e229-4373-8431-b4458568218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ray import init, rllib, tune, shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c8c-c3e1-4a6e-9f23-d9974348334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack_simulator.agents import DEFENDERS\n",
    "from attack_simulator.env import AttackSimulationEnv\n",
    "from attack_simulator.graph import AttackGraph, SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d59b78-7bc4-48c5-9d53-a0ed7a73f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPolicy(rllib.policy.Policy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "        agent_config = dict(\n",
    "            input_dim=observation_space.shape[0],\n",
    "            num_actions=action_space.n,\n",
    "            random_seed=config[\"seed\"],\n",
    "            attack_graph=config[\"env_config\"][\"attack_graph\"],\n",
    "        )\n",
    "        self._agent = DEFENDERS[config[\"agent_type\"]](agent_config)\n",
    "\n",
    "    def compute_actions(self, observations, *args, **kwargs):\n",
    "        # FIXME: use a `numpy` array as a temporary workaround for\n",
    "        #        https://github.com/ray-project/ray/issues/10100\n",
    "        return np.array([self._agent.act(obs) for obs in observations]), [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "\n",
    "def template_agent(agent_type):\n",
    "    default_config = rllib.agents.trainer.with_common_config(dict(agent_type=agent_type))\n",
    "    return rllib.agents.trainer_template.build_trainer(\n",
    "        name=agent_type,\n",
    "        default_policy=AgentPolicy,\n",
    "        default_config=default_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41ecb4-e26b-46ad-a2d6-bd09cf3b7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class AttackSimAlphaZeroEnv(AttackSimulationEnv):\n",
    "    def set_state(self, state):\n",
    "        (\n",
    "            self.simulation_time,\n",
    "            self.ttc_remaining,\n",
    "            self.attack_surface,\n",
    "            self.attack_state,\n",
    "            self.service_state,\n",
    "            self._observation,\n",
    "            self.rng,\n",
    "            self.attacker,\n",
    "        ) = deepcopy(state)\n",
    "\n",
    "    def get_state(self):\n",
    "        state = (\n",
    "            self.simulation_time,\n",
    "            self.ttc_remaining,\n",
    "            self.attack_surface,\n",
    "            self.attack_state,\n",
    "            self.service_state,\n",
    "            self._observation,\n",
    "            self.rng,\n",
    "            self.attacker,\n",
    "        )\n",
    "        return deepcopy(state)\n",
    "    \n",
    "\n",
    "class AlphaZeroWrapper(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        self.env = config[\"env_class\"](config)\n",
    "        self.action_space = self.env.action_space\n",
    "        assert isinstance(\n",
    "            self.action_space, gym.spaces.Discrete\n",
    "        ), \"AlphaZero requires a Discrete action space\"\n",
    "        shape = (self.action_space.n,)\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            dict(obs=self.env.observation_space, action_mask=gym.spaces.Box(0, 1, shape))\n",
    "        )\n",
    "        self.reward = 0\n",
    "        self.mask = np.full(shape, 1, dtype=\"int8\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.reward = 0\n",
    "        observation = self.env.reset()\n",
    "        return dict(obs=observation, action_mask=self.mask)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.reward += reward\n",
    "        reward = self.reward if done else 0\n",
    "        return dict(obs=observation, action_mask=self.mask), reward, done, info\n",
    "\n",
    "    def set_state(self, state):\n",
    "        env_state, self.reward = state\n",
    "        self.env.set_state(env_state)\n",
    "        return dict(obs=self.env.observation, action_mask=self.mask)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.env.get_state(), self.reward\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        self.env.render(mode)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397d725-6f0e-43c4-9fc7-5f13e41e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'contrib/AlphaZero' does NOT appear to work without its custom dense model\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "\n",
    "rllib.models.ModelCatalog.register_custom_model(\"alpha_zero_dense_model\", DenseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f262a77-9f87-436a-8208-a3f1bad01b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"/var/run/secrets/kubernetes.io\"):  # inside k8s pod\n",
    "    args = dict(address=\"auto\")\n",
    "else:\n",
    "    # listen on all interfaces inside a container for port-forwarding to work\n",
    "    dashboard_host = \"0.0.0.0\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "    args = dict(num_cpus=4, dashboard_host=dashboard_host)\n",
    "\n",
    "# ALTERNATIVE: use the \"Ray client\" to connect to a remote cluster\n",
    "# Unfortunately, JupyterNotebookReporter displays an object reference\n",
    "# <IPython.core.display.HTML object> instead of content...\n",
    "# --- --- ---\n",
    "#\n",
    "# from ray.util.client import worker\n",
    "#\n",
    "# worker.INITIAL_TIMEOUT_SEC = worker.MAX_TIMEOUT_SEC = 1\n",
    "#\n",
    "# ray_client_server = 'host.docker.internal' if os.path.exists(\"/.dockerenv\") else '127.0.0.1'\n",
    "# try:\n",
    "#     init(address=f'ray://{ray_client_server}:10001')\n",
    "# except Connection Error:\n",
    "#     pass  # TODO: try something else..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc9560-251b-40bf-9f00-6a0846514d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_types = [\"contrib/AlphaZero\", \"R2D2\", \"rule-based\", \"random\"]\n",
    "graphs = [AttackGraph(dict(graph_size=size)) for size in SIZES]\n",
    "seeds = [0, 1, 2, 3, 6, 7, 11, 28, 42, 1337]\n",
    "iterations = 10\n",
    "rollouts = 10\n",
    "\n",
    "common_config = dict(\n",
    "    # log_level='DEBUG',\n",
    "    framework=\"torch\",\n",
    "    env=AttackSimulationEnv,\n",
    "    env_config=dict(attack_graph=tune.grid_search(graphs)),\n",
    "    seed=tune.grid_search(seeds),\n",
    "    # common evaluation settings\n",
    "    evaluation_num_workers=1,\n",
    "    evaluation_config=dict(\n",
    "        explore=False,\n",
    "        # workaround for a bug in RLLib (https://github.com/ray-project/ray/issues/17921)\n",
    "        replay_sequence_length=-1,\n",
    "    ),\n",
    "    evaluation_num_episodes=rollouts,\n",
    ")\n",
    "train_and_eval_config = dict(\n",
    "    common_config,\n",
    "    model=dict(use_lstm=True),\n",
    "    num_workers=4,\n",
    "    # evaluation at the end\n",
    "    evaluation_interval=iterations,\n",
    ")\n",
    "eval_only_config = dict(\n",
    "    common_config,\n",
    "    # evaluation ONLY: avoid MultiGPU optimizer, set all relevant sizes to 0\n",
    "    simple_optimizer=True,\n",
    "    num_workers=0,\n",
    "    train_batch_size=0,\n",
    "    rollout_fragment_length=0,\n",
    "    timesteps_per_iteration=0,\n",
    "    # evaluation at the end\n",
    "    evaluation_interval=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8324c3a-945d-48de-9b78-817219bc8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "    \"config.env_config.agent_type\": \"Agent\",\n",
    "    \"config.env_config.attack_graph\": \"graph\",\n",
    "    \"evaluation.hist_stats.episode_reward\": \"returns\",\n",
    "    \"evaluation.hist_stats.episode_lengths\": \"lengths\",\n",
    "}\n",
    "\n",
    "def postprocess(results_df):\n",
    "    df = results_df[rename.keys()].rename(columns=rename)\n",
    "    df.dropna(inplace=True)  # remove `NaN` evaluation results from failed trials \n",
    "    df[\"Graph size\"] = df[\"graph\"].apply(lambda g: g.num_attacks)\n",
    "    del df[\"graph\"]\n",
    "    df[\"tuple\"] = df.apply(lambda t: list(zip(t.returns, t.lengths)), axis=\"columns\")\n",
    "    del df[\"returns\"]\n",
    "    del df[\"lengths\"]\n",
    "    df = df.explode(\"tuple\", ignore_index=True)\n",
    "    df[[\"Returns\", \"Episode lengths\"]] = df[\"tuple\"].tolist()\n",
    "    del df[\"tuple\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8a4c-6aa2-4e41-b86c-e622f9b2c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(savename):\n",
    "    init(**args)\n",
    "\n",
    "    frames = []\n",
    "    for agent_type in agent_types:\n",
    "        if agent_type in DEFENDERS:\n",
    "            agent = template_agent(agent_type)\n",
    "            config = eval_only_config\n",
    "            stop = dict(training_iteration=0)\n",
    "        else:\n",
    "            agent = agent_type\n",
    "            config = train_and_eval_config\n",
    "            if agent_type == \"contrib/AlphaZero\":\n",
    "                config[\"env_config\"].update(env_class=AttackSimAlphaZeroEnv)\n",
    "                config.update(\n",
    "                    env=AlphaZeroWrapper,\n",
    "                    model=dict(custom_model=\"alpha_zero_dense_model\"),\n",
    "                    rollout_fragment_length=32,\n",
    "                    train_batch_size=640,\n",
    "                    buffer_size=512,\n",
    "                )\n",
    "            stop = dict(training_iteration=iterations)  # TODO: additional stopping criteria?\n",
    "\n",
    "        config[\"env_config\"].update(agent_type=agent_type)\n",
    "        results = tune.run(\n",
    "            agent,\n",
    "            config=config,\n",
    "            stop=stop,\n",
    "            max_failures=3,\n",
    "            queue_trials=True,\n",
    "            raise_on_failed_trial=False,\n",
    "            progress_reporter=tune.JupyterNotebookReporter(overwrite=True),\n",
    "        )\n",
    "        frames.append(results.results_df)\n",
    "\n",
    "    shutdown()\n",
    "    results_df = pd.concat(frames, ignore_index=True)\n",
    "    df = postprocess(results_df)\n",
    "    df.to_csv(savename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf8dc-705f-4678-a445-9d54edb194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = \"returns-tune-eval.csv\"\n",
    "\n",
    "df = generate(savename) if not os.path.exists(savename) else pd.read_csv(savename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984bbe-82c6-4dca-8436-6d78cf97d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", rc={\"figure.figsize\": (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5424a47-3486-472b-a604-779d30ca7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Returns\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Returns vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2d787-49db-4d90-a8ae-a69d97c085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Episode lengths\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Episode lengths vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adcedc-cd95-4beb-9688-2501ffb83167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_columns\", 32)\n",
    "df.groupby(\"Agent\").describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
