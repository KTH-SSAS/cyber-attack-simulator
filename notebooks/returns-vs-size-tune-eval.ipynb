{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda8002-e229-4373-8431-b4458568218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ray import init, rllib, tune, shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c8c-c3e1-4a6e-9f23-d9974348334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack_simulator.agents import DEFENDERS\n",
    "from attack_simulator.alphazero_env import AttackSimulationAlphaZeroEnv\n",
    "from attack_simulator.env import AttackSimulationEnv\n",
    "from attack_simulator.graph import AttackGraph, SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d59b78-7bc4-48c5-9d53-a0ed7a73f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPolicy(rllib.policy.Policy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "        agent_config = dict(\n",
    "            input_dim=observation_space.shape[0],\n",
    "            num_actions=action_space.n,\n",
    "            random_seed=config[\"seed\"],\n",
    "            attack_graph=config[\"env_config\"][\"attack_graph\"],\n",
    "        )\n",
    "        self._agent = DEFENDERS[config[\"agent_type\"]](agent_config)\n",
    "\n",
    "    def compute_actions(self, observations, *args, **kwargs):\n",
    "        # FIXME: use a `numpy` array as a temporary workaround for\n",
    "        #        https://github.com/ray-project/ray/issues/10100\n",
    "        return np.array([self._agent.act(obs) for obs in observations]), [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        return {}\n",
    "\n",
    "\n",
    "def template_agent(agent_type):\n",
    "    default_config = rllib.agents.trainer.with_common_config(dict(agent_type=agent_type))\n",
    "    return rllib.agents.trainer_template.build_trainer(\n",
    "        name=agent_type,\n",
    "        default_policy=AgentPolicy,\n",
    "        default_config=default_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397d725-6f0e-43c4-9fc7-5f13e41e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'contrib/AlphaZero' does NOT appear to work without its custom dense model\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "\n",
    "rllib.models.ModelCatalog.register_custom_model(\"alpha_zero_dense_model\", DenseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f262a77-9f87-436a-8208-a3f1bad01b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.client import worker\n",
    "\n",
    "worker.INITIAL_TIMEOUT_SEC = worker.MAX_TIMEOUT_SEC = 1\n",
    "\n",
    "\n",
    "def ray_init():\n",
    "    if os.path.isdir(\"/var/run/secrets/kubernetes.io\") or os.path.exists(\n",
    "        os.path.expanduser(\"~/ray_bootstrap_config.yaml\")\n",
    "    ):\n",
    "        # inside k8s pod or a cluster managed by Ray's autoscaler\n",
    "        context = init(address=\"auto\")\n",
    "    else:\n",
    "        ray_client_server = \"host.docker.internal\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "        try:\n",
    "            context = init(address=f\"ray://{ray_client_server}:10001\")\n",
    "        except ConnectionError:\n",
    "            # clean up after failed connection attempt\n",
    "            shutdown()\n",
    "            # listen on all interfaces inside a container for port-forwarding to work\n",
    "            dashboard_host = \"0.0.0.0\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "            context = init(num_cpus=4, dashboard_host=dashboard_host)\n",
    "    print(\"\\x1b[33;1m\", context, \"\\x1b[m\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc9560-251b-40bf-9f00-6a0846514d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_types = [\"contrib/AlphaZero\", \"R2D2\", \"rule-based\", \"random\"]\n",
    "graphs = [AttackGraph(dict(graph_size=size)) for size in SIZES]\n",
    "seeds = [0, 1, 2, 3, 6, 7, 11, 28, 42, 1337]\n",
    "iterations = 10\n",
    "rollouts = 10\n",
    "\n",
    "common_config = dict(\n",
    "    # log_level='DEBUG',\n",
    "    framework=\"torch\",\n",
    "    env=AttackSimulationEnv,\n",
    "    env_config=dict(attack_graph=tune.grid_search(graphs)),\n",
    "    seed=tune.grid_search(seeds),\n",
    "    # common evaluation settings\n",
    "    evaluation_num_workers=1,\n",
    "    evaluation_config=dict(\n",
    "        explore=False,\n",
    "        # workaround for a bug in RLLib (https://github.com/ray-project/ray/issues/17921)\n",
    "        replay_sequence_length=-1,\n",
    "    ),\n",
    "    evaluation_num_episodes=rollouts,\n",
    ")\n",
    "train_and_eval_config = dict(\n",
    "    common_config,\n",
    "    num_workers=2,\n",
    "    # evaluation at the end\n",
    "    evaluation_interval=iterations,\n",
    ")\n",
    "eval_only_config = dict(\n",
    "    common_config,\n",
    "    # evaluation ONLY: avoid MultiGPU optimizer, set all relevant sizes to 0\n",
    "    simple_optimizer=True,\n",
    "    num_workers=0,\n",
    "    train_batch_size=0,\n",
    "    rollout_fragment_length=0,\n",
    "    timesteps_per_iteration=0,\n",
    "    # evaluation at the end\n",
    "    evaluation_interval=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8324c3a-945d-48de-9b78-817219bc8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "    \"config.env_config.agent_type\": \"Agent\",\n",
    "    \"config.env_config.attack_graph\": \"graph\",\n",
    "    \"evaluation.hist_stats.episode_reward\": \"returns\",\n",
    "    \"evaluation.hist_stats.episode_lengths\": \"lengths\",\n",
    "}\n",
    "\n",
    "\n",
    "def postprocess(results_df):\n",
    "    df = results_df[rename.keys()].rename(columns=rename)\n",
    "    df.dropna(inplace=True)  # remove `NaN` evaluation results from failed trials\n",
    "    df[\"Graph size\"] = df[\"graph\"].apply(lambda g: g.num_attacks)\n",
    "    del df[\"graph\"]\n",
    "    df[\"tuple\"] = df.apply(lambda t: list(zip(t.returns, t.lengths)), axis=\"columns\")\n",
    "    del df[\"returns\"]\n",
    "    del df[\"lengths\"]\n",
    "    df = df.explode(\"tuple\", ignore_index=True)\n",
    "    df[[\"Returns\", \"Episode lengths\"]] = df[\"tuple\"].tolist()\n",
    "    del df[\"tuple\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8a4c-6aa2-4e41-b86c-e622f9b2c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(savename):\n",
    "    ray_init()\n",
    "\n",
    "    frames = []\n",
    "    for agent_type in agent_types:\n",
    "        if agent_type in DEFENDERS:\n",
    "            agent = template_agent(agent_type)\n",
    "            config = dict(eval_only_config)\n",
    "            stop = dict(training_iteration=0)\n",
    "        else:\n",
    "            agent = agent_type\n",
    "            if agent_type == \"contrib/AlphaZero\":\n",
    "                config = dict(\n",
    "                    train_and_eval_config,\n",
    "                    env=AttackSimulationAlphaZeroEnv,\n",
    "                    model=dict(custom_model=\"alpha_zero_dense_model\"),\n",
    "                    rollout_fragment_length=32,\n",
    "                    train_batch_size=640,\n",
    "                    buffer_size=512,\n",
    "                )\n",
    "            else:\n",
    "                config = dict(train_and_eval_config, model=dict(use_lstm=True))\n",
    "\n",
    "            stop = dict(training_iteration=iterations)  # TODO: additional stopping criteria?\n",
    "\n",
    "        config[\"env_config\"].update(agent_type=agent_type)\n",
    "        results = tune.run(\n",
    "            agent,\n",
    "            config=config,\n",
    "            stop=stop,\n",
    "            max_failures=3,\n",
    "            queue_trials=True,\n",
    "            raise_on_failed_trial=False,\n",
    "            progress_reporter=tune.JupyterNotebookReporter(overwrite=True),\n",
    "            sync_config=tune.SyncConfig(sync_to_driver=False),\n",
    "        )\n",
    "        frames.append(results.results_df)\n",
    "\n",
    "    shutdown()\n",
    "    results_df = pd.concat(frames, ignore_index=True)\n",
    "    df = postprocess(results_df)\n",
    "    df.to_csv(savename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf8dc-705f-4678-a445-9d54edb194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = \"returns-tune-eval.csv\"\n",
    "\n",
    "df = generate(savename) if not os.path.exists(savename) else pd.read_csv(savename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984bbe-82c6-4dca-8436-6d78cf97d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", rc={\"figure.figsize\": (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5424a47-3486-472b-a604-779d30ca7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Returns\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Returns vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2d787-49db-4d90-a8ae-a69d97c085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Episode lengths\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Episode lengths vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adcedc-cd95-4beb-9688-2501ffb83167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_columns\", 32)\n",
    "df.groupby(\"Agent\").describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
