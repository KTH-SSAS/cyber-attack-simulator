{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda8002-e229-4373-8431-b4458568218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from ray import init, rllib, tune, shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c8c-c3e1-4a6e-9f23-d9974348334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack_simulator.agents import DEFENDERS\n",
    "from attack_simulator.env import AttackSimulationEnv\n",
    "from attack_simulator.graph import AttackGraph, SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d59b78-7bc4-48c5-9d53-a0ed7a73f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPolicy(rllib.policy.Policy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "        agent_config = dict(\n",
    "            input_dim=observation_space.shape[0],\n",
    "            num_actions=action_space.n,\n",
    "            random_seed=config[\"seed\"],\n",
    "            attack_graph=config[\"env_config\"][\"attack_graph\"],\n",
    "        )\n",
    "        self._agent = DEFENDERS[config[\"agent_type\"]](agent_config)\n",
    "\n",
    "    def compute_actions(self, observations, *args, **kwargs):\n",
    "        return [self._agent.act(obs) for obs in observations], [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "\n",
    "def instantiate_agent(agent_type, config):\n",
    "    default_config = rllib.agents.trainer.with_common_config(dict(agent_type=agent_type))\n",
    "    return rllib.agents.trainer_template.build_trainer(\n",
    "        name=agent_type,\n",
    "        default_policy=AgentPolicy,\n",
    "        default_config=default_config,\n",
    "    )(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41ecb4-e26b-46ad-a2d6-bd09cf3b7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "class AlphaZeroWrapper(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        self.env = config[\"env_class\"](config)\n",
    "        self.action_space = self.env.action_space\n",
    "        assert isinstance(\n",
    "            self.action_space, gym.spaces.Discrete\n",
    "        ), \"AlphaZero requires a Discrete action space\"\n",
    "        shape = (self.action_space.n,)\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            dict(obs=self.env.observation_space, action_mask=gym.spaces.Box(0, 1, shape))\n",
    "        )\n",
    "        self.reward = 0\n",
    "        self.mask = np.full(shape, 1, dtype=\"int8\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.reward = 0\n",
    "        observation = self.env.reset()\n",
    "        return dict(obs=observation, action_mask=self.mask)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.reward += reward\n",
    "        reward = self.reward if done else 0\n",
    "        return dict(obs=observation, action_mask=self.mask), reward, done, info\n",
    "\n",
    "    def set_state(self, state):\n",
    "        env, self.reward = state\n",
    "        self.env = deepcopy(env)\n",
    "        return dict(obs=self.env.observation, action_mask=self.mask)\n",
    "\n",
    "    def get_state(self):\n",
    "        return deepcopy(self.env), self.reward\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        self.env.render(mode)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397d725-6f0e-43c4-9fc7-5f13e41e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'contrib/AlphaZero' does NOT appear to work without its custom dense model\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "\n",
    "rllib.models.ModelCatalog.register_custom_model(\"alpha_zero_dense_model\", DenseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211eb85-4419-46a1-9848-876e074e8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RolloutAggregator:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._kwargs = kwargs\n",
    "        self._episodes = []\n",
    "\n",
    "    def begin_rollout(self):\n",
    "        self._rewards = []\n",
    "\n",
    "    def append_step(self, obs, action, next_obs, reward, done, info):\n",
    "        self._rewards.append(reward)\n",
    "\n",
    "    def end_rollout(self):\n",
    "        self._episodes.append(\n",
    "            dict(self._kwargs, episode_length=len(self._rewards), episode_reward=sum(self._rewards))\n",
    "        )\n",
    "\n",
    "    def to_df(self):\n",
    "        return pd.DataFrame(self._episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f262a77-9f87-436a-8208-a3f1bad01b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"/var/run/secrets/kubernetes.io\"):  # inside k8s pod\n",
    "    init(address=\"auto\")\n",
    "else:\n",
    "    # listen on all interfaces inside a container for port-forwarding to work\n",
    "    dashboard_host = \"0.0.0.0\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "    init(num_cpus=4, dashboard_host=dashboard_host)\n",
    "\n",
    "# ALTERNATIVE: use the \"Ray client\" to connect to a remote cluster\n",
    "# Unfortunately, JupyterNotebookReporter displays an object reference\n",
    "# <IPython.core.display.HTML object> instead of content...\n",
    "# --- --- ---\n",
    "#\n",
    "# from ray.util.client import worker\n",
    "#\n",
    "# worker.INITIAL_TIMEOUT_SEC = worker.MAX_TIMEOUT_SEC = 1\n",
    "#\n",
    "# ray_client_server = 'host.docker.internal' if os.path.exists(\"/.dockerenv\") else '127.0.0.1'\n",
    "# try:\n",
    "#     init(address=f'ray://{ray_client_server}:10001')\n",
    "# except Connection Error:\n",
    "#     pass  # TODO: try something else..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8a4c-6aa2-4e41-b86c-e622f9b2c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.rollout import rollout\n",
    "from ray.tune.utils.trainable import TrainableUtil\n",
    "from tqdm import tqdm\n",
    "\n",
    "rename = dict(\n",
    "    agent_type=\"Agent\",\n",
    "    graph_size=\"Graph size\",\n",
    "    episode_length=\"Episode lengths\",\n",
    "    episode_reward=\"Returns\",\n",
    ")\n",
    "agent_types = [\"R2D2\", \"contrib/AlphaZero\", \"rule-based\", \"random\"]\n",
    "seeds = [0, 1, 2, 3, 6, 7, 11, 28, 42, 1337]\n",
    "training_iterations = 10\n",
    "rollouts = 10\n",
    "\n",
    "\n",
    "def generate(savename):\n",
    "    frames = []\n",
    "\n",
    "    for graph_size in SIZES:\n",
    "        graph = AttackGraph(dict(graph_size=graph_size))\n",
    "\n",
    "        for seed in seeds:\n",
    "            for agent_type in agent_types:\n",
    "                config = dict(\n",
    "                    framework=\"torch\",\n",
    "                    model=dict(use_lstm=True),\n",
    "                    env=AttackSimulationEnv,\n",
    "                    env_config=dict(attack_graph=graph),\n",
    "                    seed=seed,\n",
    "                    create_env_on_driver=True,  # apparently, assumed by `rollout`\n",
    "                    num_workers=8,\n",
    "                    batch_mode=\"complete_episodes\",\n",
    "                    # log_level='DEBUG',\n",
    "                )\n",
    "                if agent_type in DEFENDERS:\n",
    "                    agent = instantiate_agent(agent_type, config)\n",
    "                else:\n",
    "                    if agent_type == \"contrib/AlphaZero\":\n",
    "                        config[\"env_config\"].update(env_class=config[\"env\"])\n",
    "                        config.update(\n",
    "                            env=AlphaZeroWrapper, model=dict(custom_model=\"alpha_zero_dense_model\")\n",
    "                        )\n",
    "                    agent_class = rllib.agents.registry.get_trainer_class(agent_type)\n",
    "                    print(agent_class, config)\n",
    "                    agent = agent_class(config=config)\n",
    "                    name = f'{agent_type.split(\"/\")[-1]}_{graph_size}_{seed}'\n",
    "                    if os.path.exists(name):\n",
    "                        checkpoint_path = TrainableUtil.get_checkpoints_paths(name).chkpt_path[0]\n",
    "                        agent.restore(checkpoint_path)\n",
    "                    else:\n",
    "                        pbar = tqdm(range(training_iterations), name)\n",
    "                        for _ in pbar:\n",
    "                            results = agent.train()\n",
    "                            # TODO: break based on results?\n",
    "                        agent.save(name)\n",
    "\n",
    "                aggregator = RolloutAggregator(agent_type=agent_type, graph_size=graph.num_attacks)\n",
    "                rollout(agent, \"AttackSim\", num_steps=0, num_episodes=rollouts, saver=aggregator)\n",
    "                frames.append(aggregator.to_df())\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True).rename(columns=rename)\n",
    "    df.to_csv(savename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a1282-533b-4d8a-bb9e-f6c67464b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture noise --no-stderr  # noqa\n",
    "\n",
    "savename = \"data.csv\"\n",
    "\n",
    "df = generate(savename) if not os.path.exists(savename) else pd.read_csv(savename, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf8dc-705f-4678-a445-9d54edb194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984bbe-82c6-4dca-8436-6d78cf97d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", rc={\"figure.figsize\": (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5424a47-3486-472b-a604-779d30ca7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Returns\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Returns vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2d787-49db-4d90-a8ae-a69d97c085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x=\"Graph size\", y=\"Episode lengths\", hue=\"Agent\", ci=\"sd\")\n",
    "g.legend(title=\"Agent\", loc=\"upper left\")\n",
    "g.set_title(\"Episode lengths vs Size (random attacker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6d9d1-2b14-4e48-a4a9-85785a112121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 32)\n",
    "df.groupby(\"Agent\").describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
