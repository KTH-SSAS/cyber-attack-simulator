{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda8002-e229-4373-8431-b4458568218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from ray import init, rllib, tune, shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c8c-c3e1-4a6e-9f23-d9974348334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attack_simulator.agents import DEFENDERS\n",
    "from attack_simulator.env import AttackSimulationEnv\n",
    "from attack_simulator.graph import AttackGraph, SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d59b78-7bc4-48c5-9d53-a0ed7a73f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class BooleanVectorPreprocessor(rllib.models.preprocessors.Preprocessor):\n",
    "    def _init_shape(self, observation_space, options=None):\n",
    "        return (len(observation_space.spaces),)\n",
    "\n",
    "    def transform(self, observation):\n",
    "        return np.array(observation)\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        space = gym.spaces.Box(0, 1, self.shape, dtype='int8')\n",
    "        space.original_space = self._obs_space\n",
    "        return space\n",
    "\n",
    "rllib.models.ModelCatalog.register_custom_preprocessor('boolean_vector', BooleanVectorPreprocessor)\n",
    "\n",
    "\n",
    "class AgentPolicy(rllib.policy.Policy):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "        agent_config = dict(\n",
    "            input_dim=observation_space.shape[0], # same as len(observation_space.original_space.spaces)\n",
    "            num_actions=action_space.n,\n",
    "            random_seed=config['seed'],\n",
    "            attack_graph=config['env_config']['attack_graph'],\n",
    "        )\n",
    "        self._agent = DEFENDERS[config['agent_type']](agent_config)\n",
    "\n",
    "    def compute_actions(self, observations, *args, **kwargs):\n",
    "        return [self._agent.act(obs) for obs in observations], [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def instantiate_agent(agent_type, config):\n",
    "    default_config = rllib.agents.trainer.with_common_config(dict(config, agent_type=agent_type, model=dict(custom_preprocessor='boolean_vector'), env_class=config['env']))\n",
    "    return rllib.agents.trainer_template.build_trainer(\n",
    "        name=agent_type,\n",
    "        default_policy=AgentPolicy,\n",
    "        default_config=default_config,\n",
    "    )(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41ecb4-e26b-46ad-a2d6-bd09cf3b7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class AlphaZeroWrapper(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        self.env = config['env_class'](config)\n",
    "        self.action_space = self.env.action_space\n",
    "        assert isinstance(self.action_space, gym.spaces.Discrete), 'AlphaZero requires a Discrete action space'\n",
    "        shape = (self.action_space.n,)\n",
    "        self.observation_space = gym.spaces.Dict(dict(obs=self.env.observation_space, action_mask=gym.spaces.Box(0, 1, shape)))\n",
    "        self.reward = 0\n",
    "        self.mask = np.full(shape, 1, dtype='int8')\n",
    "        \n",
    "    def reset(self):\n",
    "        self.reward = 0\n",
    "        observation = self.env.reset()\n",
    "        return dict(obs=observation, action_mask=self.mask)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.reward += reward\n",
    "        reward = self.reward if done else 0\n",
    "        return dict(obs=observation, action_mask=self.mask), reward, done, info\n",
    "\n",
    "    def set_state(self, state):\n",
    "        env, self.reward = state\n",
    "        self.env = deepcopy(env)\n",
    "        return dict(obs=self.env.observation, action_mask=self.mask)\n",
    "\n",
    "    def get_state(self):\n",
    "        return deepcopy(self.env), self.reward\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode=None):\n",
    "        self.env.render(mode)\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211eb85-4419-46a1-9848-876e074e8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class RolloutAggregator:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._kwargs = kwargs\n",
    "        self._episodes = []\n",
    "\n",
    "    def begin_rollout(self):\n",
    "        self._rewards = []\n",
    "\n",
    "    def append_step(self, obs, action, next_obs, reward, done, info):\n",
    "        self._rewards.append(reward)\n",
    "\n",
    "    def end_rollout(self):\n",
    "        self._episodes.append(\n",
    "            dict(self._kwargs, episode_length=len(self._rewards), episode_reward=sum(self._rewards))\n",
    "        )\n",
    "    \n",
    "    def to_df(self):\n",
    "        return pd.DataFrame(self._episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f262a77-9f87-436a-8208-a3f1bad01b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('/var/run/secrets/kubernetes.io'):  # inside k8s pod\n",
    "    args = dict(address='auto')\n",
    "else:  # listen on all interfaces inside a container for port-forwarding to work\n",
    "    dashboard_host = \"0.0.0.0\" if os.path.exists(\"/.dockerenv\") else \"127.0.0.1\"\n",
    "    args = dict(num_cpus=4, dashboard_host=dashboard_host)\n",
    "\n",
    "init(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8a4c-6aa2-4e41-b86c-e622f9b2c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.rollout import rollout\n",
    "from tqdm import tqdm\n",
    "\n",
    "agent_types = ['R2D2', 'contrib/AlphaZero', 'rule-based', 'random']\n",
    "seeds = [0, 1, 2, 3, 6, 7, 11, 28, 42, 1337]\n",
    "training_iterations = 10\n",
    "rollouts = 10\n",
    "\n",
    "def generate(savename):\n",
    "    frames = []\n",
    "\n",
    "    for graph_size in SIZES:\n",
    "        graph = AttackGraph(dict(graph_size=graph_size))\n",
    "\n",
    "        for seed in seeds:\n",
    "            config = dict(\n",
    "                framework='torch',\n",
    "                model=dict(use_lstm=True),\n",
    "                env=AttackSimulationEnv,\n",
    "                env_config=dict(attack_graph=graph),\n",
    "                seed=seed,\n",
    "                create_env_on_driver=True,  # apparently, assumed by `rollout`\n",
    "                num_workers=8, # for auto-scaling  # use 0 to run on driver for debugging\n",
    "                batch_mode='complete_episodes',\n",
    "            )\n",
    "            for agent_type in agent_types:\n",
    "                if agent_type in DEFENDERS:\n",
    "                    agent = instantiate_agent(agent_type, config)\n",
    "                else:\n",
    "                    if agent_type == 'contrib/AlphaZero':\n",
    "                        config['env_config'].update(env_class=config['env'])\n",
    "                        config.update(env=AlphaZeroWrapper)\n",
    "                    agent = rllib.agents.registry.get_trainer_class(agent_type)(config=config)\n",
    "                    name = f'{agent_type.split(\"/\")[-1]}_{graph_size}_{seed}'\n",
    "                    if os.path.exists(name):\n",
    "                        checkpoint_path = tune.utils.trainable.TrainableUtil.get_checkpoints_paths(name).chkpt_path[0]\n",
    "                        agent.restore(checkpoint_path)\n",
    "                    else:\n",
    "                        pbar = tqdm(range(training_iterations), f'{graph_size:13.13s} [{seed: 6d}] {agent_type:11.11s}')\n",
    "                        for _ in pbar:\n",
    "                            results = agent.train()\n",
    "                            # TODO: break based on results?\n",
    "                        agent.save(name)\n",
    "\n",
    "                aggregator = RolloutAggregator(agent_type=agent_type, graph_size=graph.num_attacks)\n",
    "                rollout(agent, 'AttackSimulator', num_steps=0, num_episodes=rollouts, saver=aggregator)\n",
    "                frames.append(aggregator.to_df())\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True).rename(columns=dict(agent_type='Agent', graph_size='Graph size', episode_length='Episode lengths', episode_reward='Returns'))\n",
    "    df.to_csv(savename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a1282-533b-4d8a-bb9e-f6c67464b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture noise --no-stderr\n",
    "\n",
    "savename = 'data.csv'\n",
    "\n",
    "df = generate(savename) if not os.path.exists(savename) else pd.read_csv(savename, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf8dc-705f-4678-a445-9d54edb194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984bbe-82c6-4dca-8436-6d78cf97d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid', rc={'figure.figsize': (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5424a47-3486-472b-a604-779d30ca7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x='Graph size', y='Returns', hue='Agent', ci='sd')\n",
    "g.legend(title='Agent', loc='upper left')\n",
    "g.set_title('Returns vs Size (random attacker)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2d787-49db-4d90-a8ae-a69d97c085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data=df, x='Graph size', y='Episode lengths', hue='Agent', ci='sd')\n",
    "g.legend(title='Agent', loc='upper left')\n",
    "g.set_title('Episode lengths vs Size (random attacker)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6d9d1-2b14-4e48-a4a9-85785a112121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 32)\n",
    "df.groupby('Agent').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e5261-f9d0-40b5-92ea-29dfbc439ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
